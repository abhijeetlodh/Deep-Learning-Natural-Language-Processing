{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JR_KeqZfNWmd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zbdRNwlNdJg"
   },
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4reJ8jgJMbY",
    "outputId": "7bd3798c-a565-4338-bfb3-a9652036d440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "\n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "\n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnisq8xnNf4Z"
   },
   "source": [
    "# Reading data without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opOnuyvNKEBP",
    "outputId": "5d98f4a3-e7e7-4590-ed2e-07eacd9de48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 138.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Big guns ease through in San Jose  Top-seeded Amer\n",
      "Example words (end):  rs such as Opera, Safari, Amaya and even Netscape.\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "\n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "\n",
    "    print(\"Reading files\")\n",
    "\n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "\n",
    "        for fi, f in enumerate(files):\n",
    "\n",
    "            # We don't read the README file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "\n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "\n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "\n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "\n",
    "                    story.append(row.strip())\n",
    "\n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)\n",
    "                # Add that to the list\n",
    "                news_stories.append(story)\n",
    "\n",
    "        print('', end='\\r')\n",
    "\n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "\n",
    "\n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftYdQjOXNlmk"
   },
   "source": [
    "# Building a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQJ8b2mrKcTX",
    "outputId": "46b2898d-11c0-419e-ad17-8d3892189e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzGLT0HkNovS"
   },
   "source": [
    "# Exploring the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dtLbQLTKe8A",
    "outputId": "cd169bf5-bc04-441b-f774-118dc5c1fdd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32360\n",
      "\n",
      "Words at the top\n",
      "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
      "\n",
      "Words at the bottom\n",
      "\t {'eos': 32350, '300d': 32351, 'novatech': 32352, 'banded': 32353, \"giant's\": 32354, 'incarnations': 32355, 'charted': 32356, 'garner': 32357, 'browsed': 32358, 'amaya': 32359}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuHX7b5YNs2R"
   },
   "source": [
    "# Building a tokenizer (refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yc0AQZE5Ke1Q",
    "outputId": "0a0c82a3-9b4f-4d72-deb6-cecf44367aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab-1,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', oov_token='',\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wpPK9mTNw_R"
   },
   "source": [
    "# Checking the results of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9I_XdGLKxrt",
    "outputId": "b4fc3497-8e41-499f-eb8a-3ba2bba2118c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Big guns ease through in San Jose  Top-seeded Americans Andy Roddick and Andre Agassi survived minor\n",
      "Sequence IDs: [229, 6243, 2907, 175, 7, 1610, 1944, 146, 4501, 2795, 982, 1016, 5, 3596, 2670, 5321, 5322]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {news_stories[0][:100]}\")\n",
    "print(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2Q-YgqPN0lL"
   },
   "source": [
    "# Converting all articles to word ID sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EK0aGK3FKz-Z"
   },
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVg-acv9N1tH"
   },
   "source": [
    "# Generating skip-grams from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2mCFEOdK2bD",
    "outputId": "cf09c935-53e8-49d2-815a-97dfe973f058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample phrase: big guns ease through in\n",
      "Sample word IDs: [229, 6243, 2907, 175, 7]\n",
      "\n",
      "Sample skip-grams\n",
      "\tInput: [229, 6243] (['big', 'guns']) / Label: 1\n",
      "\tInput: [6243, 229] (['guns', 'big']) / Label: 1\n",
      "\tInput: [6243, 2907] (['guns', 'ease']) / Label: 1\n",
      "\tInput: [2907, 6243] (['ease', 'guns']) / Label: 1\n",
      "\tInput: [2907, 175] (['ease', 'through']) / Label: 1\n",
      "\tInput: [175, 2907] (['through', 'ease']) / Label: 1\n",
      "\tInput: [175, 7] (['through', 'in']) / Label: 1\n",
      "\tInput: [7, 175] (['in', 'through']) / Label: 1\n",
      "\tInput: [6243, 7384] (['guns', 'oecd']) / Label: 0\n",
      "\tInput: [229, 1316] (['big', \"party's\"]) / Label: 0\n",
      "\tInput: [7, 10735] (['in', 'exel']) / Label: 0\n",
      "\tInput: [6243, 11752] (['guns', 'conspired']) / Label: 0\n",
      "\tInput: [2907, 7976] (['ease', 'advertisers']) / Label: 0\n",
      "\tInput: [175, 7833] (['through', 'faltering']) / Label: 0\n",
      "\tInput: [2907, 4232] (['ease', 'satisfied']) / Label: 0\n",
      "\tInput: [175, 12143] (['through', 'segment']) / Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample_word_ids = news_sequences[0][:5]\n",
    "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
    "print(f\"Sample phrase: {sample_phrase}\")\n",
    "print(f\"Sample word IDs: {sample_word_ids}\\n\")\n",
    "\n",
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sample_word_ids,\n",
    "    vocabulary_size=n_vocab,\n",
    "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "print(\"Sample skip-grams\")\n",
    "\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz9Q0XbaN7tg"
   },
   "source": [
    "# Generating negative candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBxU2AQwLDfP",
    "outputId": "a5ab40a9-986d-4ead-e34c-404ed3845097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample: [[6243]]\n",
      "Negative samples: [    0    32   100     4     5     9   503    84    11 11003]\n",
      "true_expected_count: [[0.00018318]]\n",
      "sampled_expected_count: [5.60863376e-01 3.36246341e-02 1.12128174e-02 1.89874768e-01\n",
      " 1.62862480e-01 1.03782803e-01 2.26512621e-03 1.32984249e-02\n",
      " 8.78463238e-02 1.03946564e-04]\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sample_word_ids,\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size, negative_samples=0, shuffle=False,\n",
    ")\n",
    "\n",
    "inputs, labels = np.array(inputs), np.array(labels)\n",
    "\n",
    "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
    "    # A true context word that appears in the context of the target\n",
    "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
    "    num_true=1, # number of true words per example\n",
    "    num_sampled=10,\n",
    "    unique=True,\n",
    "    range_max=n_vocab,\n",
    "    name=\"negative_sampling\"\n",
    ")\n",
    "\n",
    "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
    "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
    "print(f\"true_expected_count: {true_expected_count}\")\n",
    "print(f\"sampled_expected_count: {sampled_expected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wB9-0c3wLZuP"
   },
   "outputs": [],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTTWdjAKN_he"
   },
   "source": [
    "# Generating data (positive + negative candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mlDv5gcLSdS",
    "outputId": "7cdb1473-4784-4348-f4c0-4d73c981fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 608, 7739, 2673,   80, 2400,  851, 1529,   24,   86,  608]), array([  130,    36, 11528,     2,  2068,   893,  5057,   782,     3,\n",
      "         201]))\n",
      "[1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
    "\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequences[si],\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0.0,\n",
    "            shuffle=False,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        targets, contexts, labels = [], [], []\n",
    "\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1,\n",
    "              num_sampled=negative_samples,\n",
    "              unique=True,\n",
    "              range_max=vocab_size,\n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            context = tf.concat(\n",
    "                [tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
    "                axis=0\n",
    "            )\n",
    "\n",
    "            label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.extend([target_word]*(negative_samples+1))\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "        contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
    "\n",
    "        assert contexts.shape[0] == targets.shape[0]\n",
    "        assert contexts.shape[0] == labels.shape[0]\n",
    "\n",
    "        # If seed is not provided, generate a random one\n",
    "        if not seed:\n",
    "            seed = random.randint(0, 10e6)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(contexts)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(targets)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "        for eg_id_start in range(0, contexts.shape[0], batch_size):\n",
    "            yield (\n",
    "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
    "                contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]\n",
    "\n",
    "\n",
    "news_skip_gram_gen = skip_gram_data_generator(\n",
    "    news_sequences, 4, 10, 5, n_vocab\n",
    ")\n",
    "\n",
    "for btc, bl in news_skip_gram_gen:\n",
    "\n",
    "    print(btc)\n",
    "    print(bl)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD5VaEtjP9UG"
   },
   "source": [
    "# Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UZvlFTadP6_T"
   },
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of n on either side of target word\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "epochs = 5 # Number of epochs to train for\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid data points randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1cX81mfQBcm"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVXhJXXMQCl3",
    "outputId": "f4a36420-2162-4377-98af-0d57a99f2ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 128)          1920128     ['context[0][0]']                \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          1920128     ['target[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['context_embedding[0][0]',      \n",
      "                                                                  'target_embedding[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,840,256\n",
      "Trainable params: 3,840,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Inputs - skipgrams() function outputs target, context in that order\n",
    "# we will use the same order\n",
    "input_1 = tf.keras.layers.Input(shape=(), name='target')\n",
    "input_2 = tf.keras.layers.Input(shape=(), name='context')\n",
    "\n",
    "# Two embeddings layers are used, one for the context and one for the target\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "# Look up outputs of the embedding layers\n",
    "target_out = target_embedding_layer(input_1)\n",
    "context_out = context_embedding_layer(input_2)\n",
    "\n",
    "# Computing the dot product between the two\n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "# Defining the model\n",
    "skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2],\n",
    "                                        outputs=out, name='skip_gram_model')\n",
    "\n",
    "# Compiling the model\n",
    "skip_gram_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                        optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "skip_gram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5gJyeBjSffO"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kWc2tMlSSNn0"
   },
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "\n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "\n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "\n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "\n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "\n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "\n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "\n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ugjSx-_SVGk"
   },
   "source": [
    "# Running the skip-gram algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C87YHHdxSTDg",
    "outputId": "fcf31d97-71a4-4127-8e8d-47e6b805b53f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "   2233/Unknown - 250s 111ms/step - loss: 0.5746 - accuracy: 0.8004election: monday, desire, atmosphere, lot\n",
      "me: lot, solution, otherwise, government\n",
      "with: but, locked, ongoing, reacting, owed\n",
      "you: they, we, don't, people, cannot\n",
      "were: ordeal, announced, alexander, take, see\n",
      "win: lot, decide, played, going\n",
      "those: use, government, set, lot\n",
      "music: game, lot, uk, built\n",
      "also: help, nothing, people, now, already\n",
      "third: uk, my, built, second, welcome\n",
      "best: lot, thing, way, parry\n",
      "him: me, them, help, something, people\n",
      "too: want, ways, do, getting, something\n",
      "some: lot, fact, favour, set\n",
      "through: embarrassing, while, cancel, weekend\n",
      "mr: said, charles, michael, says, but\n",
      "response: lot, used, game, made\n",
      "concern: called, band, lot, decide\n",
      "bush: disappearance, derby, deliver, atp, go\n",
      "illegal: number, lot, part, place\n",
      "reached: built, sharp, lot, many\n",
      "previously: presented, effort, decade, none\n",
      "successful: lot, used, create, inappropriate\n",
      "care: way, end, listen, opportunity\n",
      "building: failed, asia's, because, many\n",
      "premiership: cash, drinks, 11pm, higher, â£12m\n",
      "programs: responsibilities, made, look, going\n",
      "ready: dedicated, lot, used, solution\n",
      "j: yet, communicate, nokia, minimum\n",
      "alan: replace, canal, michael, paperwork, animal\n",
      "trust: me, it's, spend, need\n",
      "society: one, discuss, understand, sigh, informed\n",
      "\n",
      "\n",
      "2233/2233 [==============================] - 251s 112ms/step - loss: 0.5746 - accuracy: 0.8004\n",
      "Epoch: 2/5 started\n",
      "   2233/Unknown - 242s 108ms/step - loss: 0.4596 - accuracy: 0.8058election: attorney, conservatives', carmaker, background, monday\n",
      "me: tories, please, him, probably, nothing\n",
      "with: bbc's, ongoing, together, but, reopen\n",
      "you: we, they, don't, didn't, people\n",
      "were: are, was, being, am, partly\n",
      "win: spite, heartening, tackled, failing, prosper\n",
      "those: assumed, learned, cea, lovenduski, slates\n",
      "music: patchy, disks, unsubstantiated, divide, fleet\n",
      "also: already, never, now, something, i've\n",
      "third: second, cities, trader, infect, 36th\n",
      "best: romantic, venice, picks, dallas, leap\n",
      "him: them, me, people, help, actually\n",
      "too: so, still, wrong, very, getting\n",
      "some: millions, locked, confusion, fulfilled, 37\n",
      "through: catalogue, stormy, ranking, spotlight, embarrassing\n",
      "mr: said, michael, gordon, charles, tony\n",
      "response: jesus, immersive, relied, refereed, prince's\n",
      "concern: arrested, represent, ancram, gloucestershire, controversy\n",
      "bush: bush's, george, pozzebon, bowyer, disappearance\n",
      "illegal: electricity, quantities, yeah, flak, taste\n",
      "reached: passion, revelation, deployment, liable, realisation\n",
      "previously: feel, hardly, clearly, always, trying\n",
      "successful: powys, liberty, tynecastle, sport's, region\n",
      "care: p2p, revisit, shall, duchy, misery\n",
      "building: â£600, teeth, totalitarianism, bypassed, realisation\n",
      "premiership: mod, sweden, stable, 1999, slowed\n",
      "programs: reap, seller, gears, humiliation, responsibilities\n",
      "ready: everquest, relation, circuits, plug\n",
      "j: andy, captain, maloney, arsene, b\n",
      "alan: john, mr, said, michael, dr\n",
      "trust: plug, legality, deport, imply\n",
      "society: poorest, brilliant, buried, contributors, accompanying\n",
      "\n",
      "\n",
      "2233/2233 [==============================] - 242s 108ms/step - loss: 0.4596 - accuracy: 0.8058\n",
      "Epoch: 3/5 started\n",
      "   2233/Unknown - 230s 103ms/step - loss: 0.4209 - accuracy: 0.8138election: attorney, upcoming, trail, 'not, leadership\n",
      "me: please, tories, him, we'll, probably\n",
      "with: bbc's, one's, ongoing, together, following\n",
      "you: we, don't, they, didn't, doesn't\n",
      "were: are, being, partly, have, was\n",
      "win: match, milk, riyadh, detained, heartening\n",
      "those: mps, learned, rarely, bpi, refusal\n",
      "music: disks, cameras, 25th, repairs, industry's\n",
      "also: already, repeatedly, apparently, never, they've\n",
      "third: second, fourth, fifth, 1500m, 10th\n",
      "best: category, artist, romantic, singled, contributions\n",
      "him: them, me, something, help, strongly\n",
      "too: so, very, quite, really, entirely\n",
      "some: millions, confusion, gprs, punters, i'll\n",
      "through: driving, catalogue, closer, onto, summer's\n",
      "mr: gordon, tony, says, said, jack\n",
      "response: kujda, dre's, relied, compiled, bribes\n",
      "concern: trimming, reservations, interviews, evolved, electorate\n",
      "bush: bush's, george, bowyer, macfarlane, rallied\n",
      "illegal: roughly, electricity, seamless, indonesia's, consuming\n",
      "reached: revolutionise, stating, fbi's, realisation, sponsorship\n",
      "previously: feel, actually, quite, i've, closely\n",
      "successful: benefited, sibneft, afi, sophistication, trimming\n",
      "care: p2p, fix, regaining, bundesbank, turbulent\n",
      "building: goodbye, barriers, mblox, terrified, refreshing\n",
      "premiership: quake, nation, poorest, administrative, strongest\n",
      "programs: embracing, slack, tool, agenda, minidisc\n",
      "ready: pleasant, convenient, stripped, realistic, shouldn't\n",
      "j: andy, prop, john, wife, o'kelly\n",
      "alan: john, dr, said, michael, scott\n",
      "trust: gibraltarians, you', everquest, legality, allegiances\n",
      "society: administrative, mini, customs, hulk, decline\n",
      "\n",
      "\n",
      "2233/2233 [==============================] - 230s 103ms/step - loss: 0.4209 - accuracy: 0.8138\n",
      "Epoch: 4/5 started\n",
      "   2233/Unknown - 253s 113ms/step - loss: 0.3908 - accuracy: 0.8229election: campaign, attorney, guides, november's, upcoming\n",
      "me: him, please, tories, them, don't\n",
      "with: one's, constructive, ongoing, exhibitor, tough\n",
      "you: we, i'll, don't, they, we'll\n",
      "were: are, illegally, partly, being, pending\n",
      "win: victories, upset, play, teach, match\n",
      "those: affluent, mps, learned, rarely, laptops\n",
      "music: usb, legitimate, cameras, recording, disks\n",
      "also: already, apparently, repeatedly, strongly, formally\n",
      "third: second, fourth, fifth, 10th, cities\n",
      "best: category, supporting, heirs, artist, bafta\n",
      "him: them, me, anyone, strongly, automatically\n",
      "too: very, so, really, quite, clearly\n",
      "some: confusion, serious, smoking, millions, detailed\n",
      "through: catalogue, onto, utilities, josep, gratuitous\n",
      "mr: gordon, tony, eliot, says, said\n",
      "response: ephedrine, freedoms, plight, meal, chairmanship\n",
      "concern: interviews, difficulties, hacker, idea, judges\n",
      "bush: bush's, george, clinton, macfarlane, pozzebon\n",
      "illegal: consuming, innovative, incoming, divorced, affects\n",
      "reached: revolutionise, stagnation, landlines, â£600, peril\n",
      "previously: already, i've, actually, always, nothing\n",
      "successful: benefited, popular, afi, appetite, touted\n",
      "care: p2p, fix, vision, interesting, tremors\n",
      "building: portishead, forbes, backbone, mblox, trialled\n",
      "premiership: quake, strongest, poorest, week's, refining\n",
      "programs: benefits, server, program, tool, irrigation\n",
      "ready: pleasant, realistic, shouldn't, 86, hungarian\n",
      "j: m, andy, john, brian, o'kelly\n",
      "alan: john, peter, dr, scott, michael\n",
      "trust: merely, hungarian, verizon's, everquest, crucial\n",
      "society: collection, hulk, petroleum, eastenders, decline\n",
      "\n",
      "\n",
      "2233/2233 [==============================] - 253s 113ms/step - loss: 0.3908 - accuracy: 0.8229\n",
      "Epoch: 5/5 started\n",
      "   2233/Unknown - 239s 107ms/step - loss: 0.3646 - accuracy: 0.8325election: november's, campaign, guides, labour's, campbell's\n",
      "me: him, please, anything, wouldn't, tories\n",
      "with: constructive, one's, uncertainties, tough, electro\n",
      "you: we, i'll, don't, we'll, doesn't\n",
      "were: are, illegally, partly, being, pending\n",
      "win: play, victories, hopman, derby, match\n",
      "those: affluent, rarely, reportedly, mps, laptops\n",
      "music: usb, cameras, terrestrial, snooker, legitimate\n",
      "also: already, repeatedly, formally, strongly, unsuccessfully\n",
      "third: fourth, second, fifth, replay, ninth\n",
      "best: category, heirs, supporting, adaptation, nomination\n",
      "him: me, them, anything, anyone, automatically\n",
      "too: very, extremely, so, entirely, clearly\n",
      "some: darker, smoking, experienced, serious, approximately\n",
      "through: onto, catalogue, disputed, suicide, batteries\n",
      "mr: tony, eliot, gordon, bernie, jack\n",
      "response: ephedrine, sealing, freedoms, complaints, embarking\n",
      "concern: difficulties, sophos, interviews, undertaking, idea\n",
      "bush: bush's, george, clinton, jarre, macfarlane\n",
      "illegal: affects, innovative, antennae, prohibited, tariff\n",
      "reached: revolutionise, nancy, ealing, luckily, motto\n",
      "previously: already, i've, haven't, immediately, actually\n",
      "successful: benefited, popular, vulnerable, sibneft, skilled\n",
      "care: enforced, p2p, provide, vision, longer\n",
      "building: veto, mblox, racked, totalitarianism, edit\n",
      "premiership: quake, congressional, amsterdam, 1955, week's\n",
      "programs: program, use, benefits, futures, uses\n",
      "ready: toward, surveyed, geography, pleasant, surprises\n",
      "j: m, o'kelly, p, g, beattie\n",
      "alan: john, dr, peter, walter, ian\n",
      "trust: verizon's, merely, idle, chartered, pent\n",
      "society: hulk, empire, petroleum, collection, coffers\n",
      "\n",
      "\n",
      "2233/2233 [==============================] - 239s 107ms/step - loss: 0.3646 - accuracy: 0.8325\n"
     ]
    }
   ],
   "source": [
    "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "\n",
    "    news_skip_gram_gen = skip_gram_data_generator(\n",
    "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
    "    )\n",
    "\n",
    "    skip_gram_model.fit(\n",
    "        news_skip_gram_gen, epochs=1,\n",
    "        callbacks=skipgram_validation_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
