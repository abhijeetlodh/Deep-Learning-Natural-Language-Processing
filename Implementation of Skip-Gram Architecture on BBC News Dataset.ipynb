{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the data generators with TensorFlow\n",
    "* Read data before preprocessing\n",
    "* Build a tokenizer\n",
    "* Refined tokenizer\n",
    "* Generating skip-grams from the corpus\n",
    "* Generating negative candidates\n",
    "* Generating positive and negative candidates\n",
    "* Frequencies to compute sampling table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JR_KeqZfNWmd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zbdRNwlNdJg"
   },
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4reJ8jgJMbY",
    "outputId": "7bd3798c-a565-4338-bfb3-a9652036d440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "bbc-fulltext.zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "\n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "\n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnisq8xnNf4Z"
   },
   "source": [
    "# Reading data without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opOnuyvNKEBP",
    "outputId": "5d98f4a3-e7e7-4590-ed2e-07eacd9de48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 361.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Snow Patrol feted at Irish awards  Snow Patrol wer\n",
      "Example words (end):  is years at Stradey as \"the best time of my life.\"\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "\n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "\n",
    "    print(\"Reading files\")\n",
    "\n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "\n",
    "        for fi, f in enumerate(files):\n",
    "\n",
    "            # We don't read the README file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "\n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "\n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "\n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "\n",
    "                    story.append(row.strip())\n",
    "\n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)\n",
    "                # Add that to the list\n",
    "                news_stories.append(story)\n",
    "\n",
    "        print('', end='\\r')\n",
    "\n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "\n",
    "\n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftYdQjOXNlmk"
   },
   "source": [
    "# Building a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQJ8b2mrKcTX",
    "outputId": "46b2898d-11c0-419e-ad17-8d3892189e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "# tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzGLT0HkNovS"
   },
   "source": [
    "# Exploring the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dtLbQLTKe8A",
    "outputId": "cd169bf5-bc04-441b-f774-118dc5c1fdd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32360\n",
      "\n",
      "Words at the top\n",
      "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
      "\n",
      "Words at the bottom\n",
      "\t {\"irishman's\": 32350, 'sidefooting': 32351, '110th': 32352, 'pellegrino': 32353, 'pidgeley': 32354, \"llanelli's\": 32355, 'pectoral': 32356, 'signings': 32357, 'departures': 32358, 'invigorate': 32359}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuHX7b5YNs2R"
   },
   "source": [
    "# Building a tokenizer (refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yc0AQZE5Ke1Q",
    "outputId": "0a0c82a3-9b4f-4d72-deb6-cecf44367aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab-1,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', oov_token='',\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wpPK9mTNw_R"
   },
   "source": [
    "# Checking the results of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9I_XdGLKxrt",
    "outputId": "b4fc3497-8e41-499f-eb8a-3ba2bba2118c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Snow Patrol feted at Irish awards  Snow Patrol were the big winners in Ireland's top music honours, \n",
      "Sequence IDs: [3596, 5574, 12988, 21, 760, 306, 3596, 5574, 43, 2, 229, 1143, 7, 2391, 146, 100, 2670]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {news_stories[0][:100]}\")\n",
    "print(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2Q-YgqPN0lL"
   },
   "source": [
    "# Converting all articles to word ID sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EK0aGK3FKz-Z"
   },
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVg-acv9N1tH"
   },
   "source": [
    "# Generating skip-grams from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2mCFEOdK2bD",
    "outputId": "cf09c935-53e8-49d2-815a-97dfe973f058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample phrase: snow patrol feted at irish\n",
      "Sample word IDs: [3596, 5574, 12988, 21, 760]\n",
      "\n",
      "Sample skip-grams\n",
      "\tInput: [3596, 5574] (['snow', 'patrol']) / Label: 1\n",
      "\tInput: [5574, 3596] (['patrol', 'snow']) / Label: 1\n",
      "\tInput: [5574, 12988] (['patrol', 'feted']) / Label: 1\n",
      "\tInput: [12988, 5574] (['feted', 'patrol']) / Label: 1\n",
      "\tInput: [12988, 21] (['feted', 'at']) / Label: 1\n",
      "\tInput: [21, 12988] (['at', 'feted']) / Label: 1\n",
      "\tInput: [21, 760] (['at', 'irish']) / Label: 1\n",
      "\tInput: [760, 21] (['irish', 'at']) / Label: 1\n",
      "\tInput: [12988, 2556] (['feted', 'listed']) / Label: 0\n",
      "\tInput: [760, 4793] (['irish', 'indoors']) / Label: 0\n",
      "\tInput: [5574, 12510] (['patrol', 'rescued']) / Label: 0\n",
      "\tInput: [12988, 3683] (['feted', 'puts']) / Label: 0\n",
      "\tInput: [21, 11218] (['at', 'farce']) / Label: 0\n",
      "\tInput: [3596, 7917] (['snow', 'corners']) / Label: 0\n",
      "\tInput: [5574, 6051] (['patrol', 'viewed']) / Label: 0\n",
      "\tInput: [21, 4744] (['at', 'tune']) / Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample_word_ids = news_sequences[0][:5]\n",
    "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
    "print(f\"Sample phrase: {sample_phrase}\")\n",
    "print(f\"Sample word IDs: {sample_word_ids}\\n\")\n",
    "\n",
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sample_word_ids,\n",
    "    vocabulary_size=n_vocab,\n",
    "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "print(\"Sample skip-grams\")\n",
    "\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz9Q0XbaN7tg"
   },
   "source": [
    "# Generating negative candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBxU2AQwLDfP",
    "outputId": "a5ab40a9-986d-4ead-e34c-404ed3845097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample: [[5574]]\n",
      "Negative samples: [ 5165  1600   677   753  1212 11801  3537  5949    11     0]\n",
      "true_expected_count: [[0.00018652]]\n",
      "sampled_expected_count: [2.0128522e-04 6.4935384e-04 1.5327049e-03 1.3783171e-03 8.5697579e-04\n",
      " 8.8111847e-05 2.9389292e-04 1.7476515e-04 8.3239615e-02 7.2083151e-01]\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sample_word_ids,\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size, negative_samples=0, shuffle=False,\n",
    ")\n",
    "\n",
    "inputs, labels = np.array(inputs), np.array(labels)\n",
    "\n",
    "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
    "    # A true context word that appears in the context of the target\n",
    "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
    "    num_true=1, # number of true words per example\n",
    "    num_sampled=10,\n",
    "    unique=True,\n",
    "    range_max=n_vocab,\n",
    "    name=\"negative_sampling\"\n",
    ")\n",
    "\n",
    "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
    "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
    "print(f\"true_expected_count: {true_expected_count}\")\n",
    "print(f\"sampled_expected_count: {sampled_expected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wB9-0c3wLZuP"
   },
   "outputs": [],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTTWdjAKN_he"
   },
   "source": [
    "# Generating data (positive + negative candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mlDv5gcLSdS",
    "outputId": "7cdb1473-4784-4348-f4c0-4d73c981fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1089,  1950,  3427,  4508,  1297,  2438, 11192,  4223,  3427,\n",
      "       11192]), array([   0,  385,    2,   20,    5,  159, 4099,  147,   40,  634]))\n",
      "[0 0 0 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
    "\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequences[si],\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0.0,\n",
    "            shuffle=False,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        targets, contexts, labels = [], [], []\n",
    "\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1,\n",
    "              num_sampled=negative_samples,\n",
    "              unique=True,\n",
    "              range_max=vocab_size,\n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            context = tf.concat(\n",
    "                [tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
    "                axis=0\n",
    "            )\n",
    "\n",
    "            label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.extend([target_word]*(negative_samples+1))\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "        contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
    "\n",
    "        assert contexts.shape[0] == targets.shape[0]\n",
    "        assert contexts.shape[0] == labels.shape[0]\n",
    "\n",
    "        # If seed is not provided, generate a random one\n",
    "        if not seed:\n",
    "            seed = random.randint(0, int(10e6))\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(contexts)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(targets)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "        for eg_id_start in range(0, contexts.shape[0], batch_size):\n",
    "            yield (\n",
    "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
    "                contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]\n",
    "\n",
    "\n",
    "news_skip_gram_gen = skip_gram_data_generator(\n",
    "    news_sequences, 4, 10, 5, n_vocab\n",
    ")\n",
    "\n",
    "for btc, bl in (news_skip_gram_gen):\n",
    "\n",
    "    print(btc)\n",
    "    print(bl)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD5VaEtjP9UG"
   },
   "source": [
    "# Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "UZvlFTadP6_T"
   },
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of n on either side of target word\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "epochs = 5 # Number of epochs to train for\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid data points randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1cX81mfQBcm"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVXhJXXMQCl3",
    "outputId": "f4a36420-2162-4377-98af-0d57a99f2ed2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"skip_gram_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"skip_gram_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ context             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,128</span> │ context[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,128</span> │ target[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_embeddin… │\n",
       "│                     │                   │            │ target_embedding… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ context             │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,920,128\u001b[0m │ context[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,920,128\u001b[0m │ target[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ context_embeddin… │\n",
       "│                     │                   │            │ target_embedding… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,256</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,840,256\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,256</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,840,256\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Inputs - skipgrams() function outputs target, context in that order\n",
    "# we will use the same order\n",
    "input_1 = tf.keras.layers.Input(shape=(), name='target')\n",
    "input_2 = tf.keras.layers.Input(shape=(), name='context')\n",
    "\n",
    "# Two embeddings layers are used, one for the context and one for the target\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "# Look up outputs of the embedding layers\n",
    "target_out = target_embedding_layer(input_1)\n",
    "context_out = context_embedding_layer(input_2)\n",
    "\n",
    "# Computing the dot product between the two\n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "# Defining the model\n",
    "skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2],\n",
    "                                        outputs=out, name='skip_gram_model')\n",
    "\n",
    "# Compiling the model\n",
    "skip_gram_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                        optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "skip_gram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5gJyeBjSffO"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "kWc2tMlSSNn0"
   },
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "\n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "\n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "\n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "\n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "\n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "\n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "\n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ugjSx-_SVGk"
   },
   "source": [
    "# Running the skip-gram algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C87YHHdxSTDg",
    "outputId": "fcf31d97-71a4-4127-8e8d-47e6b805b53f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "   2232/Unknown \u001b[1m97s\u001b[0m 43ms/step - accuracy: 0.8001 - loss: 0.6282election: flat, afghanistan, end, success, order\n",
      "me: him, users, things, do, authorities\n",
      "with: between, against, global, since\n",
      "you: we, they, don't, really, tories\n",
      "were: renton, about, regions, if\n",
      "win: likely, move, couple, made\n",
      "those: trying, do, terrorists, find, sit\n",
      "music: part, win, end, crucial\n",
      "also: them, never, me, properties, help\n",
      "international: attempting, shot, control, published, lay\n",
      "best: part, leadership, allowed, powerful\n",
      "him: users, things, tories, do, see\n",
      "too: way, tories, going, thought, likely\n",
      "some: trying, couple, comments, probably\n",
      "through: going, difference, example, represent\n",
      "mr: said, tony, gordon, michael, david\n",
      "looked: consider, focus, those, trying, what\n",
      "concern: due, measures, apologise, wealth, find\n",
      "computers: apologise, convention, hold, try, part\n",
      "along: designed, information, plans, powerful, handhelds\n",
      "trust: focus, likely, example, embargo\n",
      "row: convention, comment, health, millan, focus\n",
      "successful: babyshambles, constable, improve, game, take\n",
      "details: loss, powerful, tuesday, focus, part\n",
      "giving: tried, athens, challenges, looking, apologise\n",
      "actually: seek, able, do, going, want\n",
      "deutsche: sector, preventable, tennis, umbro, devastated\n",
      "fair: juninho, wealth, apply, much, going\n",
      "hollywood: reduction, command, reaching, liked, bowden\n",
      "profit: likely, part, homes, board, yes\n",
      "positive: policy, athens, catching, europeans, arrest\n",
      "parents: ability, couple, tuesday, do, likely\n",
      "\n",
      "\n",
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 43ms/step - accuracy: 0.8001 - loss: 0.6282\n",
      "Epoch: 2/5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2233/Unknown \u001b[1m98s\u001b[0m 44ms/step - accuracy: 0.8057 - loss: 0.4587election: november's, workers, labour's, success, leadership\n",
      "me: him, likhovtseva, tories, we'll, help\n",
      "with: between, against, fenice, balco, global\n",
      "you: we, they, don't, really, never\n",
      "were: are, was, already, am, have\n",
      "win: none, chance, napier, reaching, 61\n",
      "those: able, trying, seriousness, listen, deals\n",
      "music: asia, luxury, halt, uk's, total\n",
      "also: already, never, widely, had, we've\n",
      "international: juventus, customs, clash, today's, chorlton\n",
      "best: eurovision, boogeyman, formula, halt, budget'\n",
      "him: me, tories, them, help, things\n",
      "too: bit, very, roddick's, adopting, fun\n",
      "some: chocolate, draws, calibre, thrust, yourself\n",
      "through: onto, rochdale, severn, dotted\n",
      "mr: gordon, said, tony, robert, michael\n",
      "looked: inspector, definitely, confirmed, staked, fury\n",
      "concern: youngster, spying, paver, unity, idea\n",
      "computers: le, precious, tonge, withdrawal, medicine\n",
      "along: hearings, betrayal, mmorpgs, eventual, kodak\n",
      "trust: approximately, storyline, intrigued, monitored, gathered\n",
      "row: violated, archos, kinds, resolved, determination\n",
      "successful: juventus, handhelds, edged, cafes, jamaican\n",
      "details: fulfil, blackmail, castaignede, nato, valencia\n",
      "giving: challenges, flash, stemmed, boils, firing\n",
      "actually: able, probably, anything, she's, listen\n",
      "deutsche: constitutional, 1965, papers, austria's, elimination\n",
      "fair: springboks, luck, structures, hijack, mixture\n",
      "hollywood: main, voting, ifo, endured, age\n",
      "profit: outings, mile, distorted, richardson, sundance\n",
      "positive: sorry, acquisitions, rsa, attempted, constables\n",
      "parents: savoy, crass, freely, muslims, listen\n",
      "\n",
      "\n",
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 44ms/step - accuracy: 0.8057 - loss: 0.4587\n",
      "Epoch: 3/5 started\n",
      "   2232/Unknown \u001b[1m93s\u001b[0m 41ms/step - accuracy: 0.8153 - loss: 0.4156election: november's, sweden, leadership, commitments, workers\n",
      "me: him, them, we'll, you, tories\n",
      "with: between, press, against, atlantic, one's\n",
      "you: don't, we, they, we'll, certainly\n",
      "were: are, been, be, being, have\n",
      "win: 61, 34, chance, reaching, lead\n",
      "those: formally, heard, personally, awesome, enjoy\n",
      "music: legitimate, violence, coffee, phonographic, mp3\n",
      "also: already, repeatedly, previously, recently, tories\n",
      "international: 200m, football, clash, listed, meeting\n",
      "best: category, supporting, selling, counterparts, original\n",
      "him: them, me, myself, we'll, franchises\n",
      "too: very, bit, less, going, extremely\n",
      "some: foreigners, thrust, compelling, enough, calibre\n",
      "through: onto, other's, actively, it'll, drag\n",
      "mr: gordon, ken, tony, robert, jack\n",
      "looked: awesome, sufficiently, actually, definitely, converged\n",
      "concern: ebay, reservations, commitments, sympathy, â£100m\n",
      "computers: liberty, withdrawal, sundance, response, smarter\n",
      "along: slovakia, other's, elan, tuned, indefinitely\n",
      "trust: diary, approximately, telephony, monitored, arguably\n",
      "row: kinds, relate, ronnies, gaps, leap\n",
      "successful: transformation, trojan, privileges, cafes, confirmation\n",
      "details: gloomy, agreeing, casualty, vendor, darlington\n",
      "giving: blake's, rajasthan, â£400m, luxembourg, offending\n",
      "actually: probably, listen, she's, capable, definitely\n",
      "deutsche: austria's, austria, 1965, audit, marque\n",
      "fair: halloween, visitor's, hijack, commissioning, breezes\n",
      "hollywood: film, year's, sunday's, sector, voting\n",
      "profit: today's, marked, votes, dramatic, bloggies\n",
      "positive: smarter, frenzy, vauxhall, depth, maximise\n",
      "parents: petrov, prejudice, crass, listen, hook\n",
      "\n",
      "\n",
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 41ms/step - accuracy: 0.8153 - loss: 0.4156\n",
      "Epoch: 4/5 started\n",
      "   2232/Unknown \u001b[1m93s\u001b[0m 42ms/step - accuracy: 0.8248 - loss: 0.3856election: november's, leadership, guides, consulates, monsanto\n",
      "me: him, them, we'll, tories, usually\n",
      "with: one's, begin, atlantic, visuals, between\n",
      "you: we, don't, they, certainly, i'll\n",
      "were: are, have, indefinitely, locking, being\n",
      "win: 61, â£27, holland, netherlands, 34\n",
      "those: partly, formally, affair, informal, muslims\n",
      "music: legitimate, phonographic, panda, formats, violence\n",
      "also: repeatedly, previously, already, recently, correctly\n",
      "international: football, 200m, clash, meeting, championship\n",
      "best: supporting, category, counterparts, preserve, french\n",
      "him: me, them, tories, myself, franchises\n",
      "too: very, bit, less, quite, extremely\n",
      "some: spammers, better, traditionally, any, foreigners\n",
      "through: onto, motorists, other's, wandering, actively\n",
      "mr: ken, gordon, jack, ed, tony\n",
      "looked: pair, they've, sufficiently, muslims, prepared\n",
      "concern: sympathy, reservations, controversy, investigating, disappointment\n",
      "computers: liberty, flaw, impact, smarter, cult\n",
      "along: tuned, genie, other's, screaming, blank\n",
      "trust: incorporated, entrants, diary, branches, chartered\n",
      "row: ronnies, determination, unchanged, rotterdam, britpop\n",
      "successful: transformation, representative, spying, bedroom, nhs\n",
      "details: passwords, premiere, sub, auckland, nato\n",
      "giving: definite, offending, revealing, highlighting, gathers\n",
      "actually: everything, she's, they'll, able, definitely\n",
      "deutsche: austria, maintains, austria's, audit, 1965\n",
      "fair: justice, chaplin, structures, user's, hijack\n",
      "hollywood: status, prestigious, musical's, endured, novel\n",
      "profit: marked, tankan, today's, solid, monthly\n",
      "positive: conciliation, harder, reductions, frenzy, weighted\n",
      "parents: crass, muslims, complied, akamai, petrov\n",
      "\n",
      "\n",
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 42ms/step - accuracy: 0.8248 - loss: 0.3856\n",
      "Epoch: 5/5 started\n",
      "   2231/Unknown \u001b[1m93s\u001b[0m 42ms/step - accuracy: 0.8337 - loss: 0.3610election: november's, guides, broad, labour's, leadership\n",
      "me: him, them, tories, we'll, you'll\n",
      "with: atlantic, visuals, one's, begin, locking\n",
      "you: we, don't, they, exactly, wouldn't\n",
      "were: are, locking, provisionally, formally, indefinitely\n",
      "win: 61, netherlands, leipzig, hartlepool, doubled\n",
      "those: formally, muslims, rarely, investigate, partly\n",
      "music: legitimate, cameras, mp3, nomad, empire\n",
      "also: repeatedly, previously, already, absent, correctly\n",
      "international: 200m, football, championship, clash, listed\n",
      "best: preserve, supporting, counterparts, category, artist\n",
      "him: them, me, myself, tories, successfully\n",
      "too: very, bit, less, relatively, incredibly\n",
      "some: spammers, better, compelling, meaningful, extraordinary\n",
      "through: onto, fighter, motorists, wandering, specifically\n",
      "mr: gordon, ken, jack, helmut, ravi\n",
      "looked: sufficiently, accomplished, done, prepared, awesome\n",
      "concern: sympathy, â£100m, disappointment, catalyst, worry\n",
      "computers: blip, reduction, leaflets, responses, liberty\n",
      "along: tuned, belmarsh, designated, indefinitely, postings\n",
      "trust: chartered, incorporated, poor's, pent, experienced\n",
      "row: dip, retrieve, determination, phenomenon, currencies\n",
      "successful: transformation, nhs, bedroom, enthusiastic, confirmation\n",
      "details: premiere, passwords, nato, eviction, limits\n",
      "giving: intercept, amazing, definite, cuba, revealing\n",
      "actually: everything, they'll, she's, definitely, quickly\n",
      "deutsche: austria's, austria, maintains, energis, donald\n",
      "fair: shocking, justice, dysfunctional, instrumental, hijack\n",
      "hollywood: prestigious, show's, status, musical's, abortion\n",
      "profit: tankan, jobless, monthly, today's, votes\n",
      "positive: weighted, reductions, frenzy, irrespective, creating\n",
      "parents: francesca, demonstrating, complied, converged, restructure\n",
      "\n",
      "\n",
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 42ms/step - accuracy: 0.8337 - loss: 0.3610\n"
     ]
    }
   ],
   "source": [
    "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "\n",
    "    news_skip_gram_gen = skip_gram_data_generator(\n",
    "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
    "    )\n",
    "\n",
    "    skip_gram_model.fit(\n",
    "        news_skip_gram_gen, epochs=1,\n",
    "        callbacks=skipgram_validation_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
