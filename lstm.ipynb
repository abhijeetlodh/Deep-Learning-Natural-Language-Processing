{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Kbp4n8cCoUIE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmFE6tcNPKFa"
   },
   "source": [
    "# Downloading Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKGTzKRjoZLs",
    "outputId": "dc3856cd-950f-4383-87f1-ac47eb4688dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 files found.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(url, filename, download_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# Number of files and their names to download\n",
    "num_files = 100\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "# Download each file\n",
    "for fn in filenames:\n",
    "    download_data(url, fn, dir_name)\n",
    "\n",
    "# Check if all files are downloaded\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print(f\"{len(filenames)} files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovxCXSu_PHMg"
   },
   "source": [
    "# Split train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAgU-OOBofWs",
    "outputId": "0e53165b-701d-40bf-85ea-da89c25f5f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 167 files in the train dataset (e.g. ['data/148.txt', 'data/198.txt', 'data/032.txt'])\n",
      "Got 21 files in the valid dataset (e.g. ['data/055.txt', 'data/078.txt', 'data/087.txt'])\n",
      "Got 21 files in the test dataset (e.g. ['data/056.txt', 'data/101.txt', 'data/114.txt'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fix the random seed so we get the same output every time\n",
    "random_state = 54321\n",
    "\n",
    "# Define the directory name\n",
    "dir_name = 'data'\n",
    "\n",
    "# Get a list of valid filenames in the 'data' folder\n",
    "filenames = [\n",
    "    os.path.join(dir_name, f) \n",
    "    for f in os.listdir(dir_name) \n",
    "    if f.endswith('.txt') and f[:-4].isdigit() and 1 <= int(f[:-4]) <= 209\n",
    "]\n",
    "\n",
    "# First separate train and valid+test data\n",
    "train_filenames, test_and_valid_filenames = train_test_split(filenames, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Separate valid+test data to validation and test data\n",
    "valid_filenames, test_filenames = train_test_split(test_and_valid_filenames, test_size=0.5, random_state=random_state)\n",
    "\n",
    "# Print out the sizes and some sample filenames\n",
    "for subset_id, subset in zip(('train', 'valid', 'test'), (train_filenames, valid_filenames, test_filenames)):\n",
    "    print(f\"Got {len(subset)} files in the {subset_id} dataset (e.g. {subset[:3]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYurr05APDzn"
   },
   "source": [
    "# Find the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOI4hPRBohaO",
    "outputId": "14d885b3-8a01-4329-c8aa-db03cf5c4f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 unique bigrams\n"
     ]
    }
   ],
   "source": [
    "bigram_set = set()\n",
    "\n",
    "# Go through each file in the training set\n",
    "for fname in train_filenames:\n",
    "    document = [] # This will hold all the text\n",
    "    with open(fname, 'r') as f:\n",
    "        for row in f:\n",
    "            # Convert text to lower case to reduce input dimensionality\n",
    "            document.append(row.lower())\n",
    "\n",
    "        # From the list of text we have, generate one long string (containing all training stories)\n",
    "        document = \" \".join(document)\n",
    "\n",
    "        # Update the set with all bigrams found\n",
    "        bigram_set.update([document[i:i+2] for i in range(0, len(document), 2)])\n",
    "\n",
    "# Assign to a variable and print\n",
    "n_vocab = len(bigram_set)\n",
    "print(f\"Found {n_vocab} unique bigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqisknU-O-ok"
   },
   "source": [
    "# Reading data and generate batches for the model (tf.data API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1XtENSo7oyAe"
   },
   "outputs": [],
   "source": [
    "def generate_tf_dataset(filenames, ngram_width, window_size, batch_size, shuffle=False):\n",
    "    \"\"\" Generate batched data from a list of files speficied \"\"\"\n",
    "\n",
    "    # Read the data found in the documents\n",
    "    documents = []\n",
    "    for f in filenames:\n",
    "        doc = tf.io.read_file(f)\n",
    "        doc = tf.strings.ngrams(    # Generate ngrams from the string\n",
    "            tf.strings.bytes_split(    # Create a list of chars from a string\n",
    "                tf.strings.regex_replace(    # Replace new lines with space\n",
    "                    tf.strings.lower(    # Convert string to lower case\n",
    "                        doc\n",
    "                    ), \"\\n\", \" \"\n",
    "                )\n",
    "            ),\n",
    "            ngram_width, separator=''\n",
    "        )\n",
    "        documents.append(doc.numpy().tolist())\n",
    "\n",
    "    # documents is a list of list of strings, where each string is a story\n",
    "    # From that we generate a ragged tensor\n",
    "    documents = tf.ragged.constant(documents)\n",
    "    # Create a dataset where each row in the ragged tensor would be a sample\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n",
    "    # We need to perform a quick transformation - tf.strings.ngrams would generate\n",
    "    # all the ngrams (e.g. abcd -> ab, bc, cd) with overlap, however for our data\n",
    "    # we do not need the overlap, so we need to skip the overlapping ngrams\n",
    "    # the following line does that\n",
    "    doc_dataset = doc_dataset.map(lambda x: x[::ngram_width])\n",
    "\n",
    "    # Here we are using a window function to generate windows from text\n",
    "    # For a text sequence with window_size 3 and shift 1 you get\n",
    "    # e.g. ab, cd, ef, gh, ij, ... -> [ab, cd, ef], [cd, ef, gh], [ef, gh, ij], ...\n",
    "    # each of these windows is a single training sequence for our model\n",
    "    doc_dataset = doc_dataset.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "            x\n",
    "        ).window(\n",
    "            size=window_size+1, shift=int(window_size * 0.75)\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(window_size+1, drop_remainder=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # From each windowed sequence we generate input and target tuple\n",
    "    # e.g. [ab, cd, ef] -> ([ab, cd], [cd, ef])\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "\n",
    "    # Shuffle the data if required\n",
    "    doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*10) if shuffle else doc_dataset\n",
    "\n",
    "    # Batch the data\n",
    "    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Return the data\n",
    "    return doc_dataset\n",
    "\n",
    "\n",
    "ngram_length = 2\n",
    "batch_size = 128\n",
    "window_size = 128\n",
    "\n",
    "train_ds = generate_tf_dataset(train_filenames, ngram_length, window_size, batch_size, shuffle=True)\n",
    "valid_ds = generate_tf_dataset(valid_filenames, ngram_length, window_size, batch_size)\n",
    "test_ds = generate_tf_dataset(test_filenames, ngram_length, window_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXiqs_DBO9Ga"
   },
   "source": [
    "# Generate few samples from the dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kq5vnKRRo4Xf",
    "outputId": "0372fe43-7dc6-4b0b-adec-11174a8d71f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'on' b'e ' b'af' b'te' b'rn' b'oo' b'n ' b'th' b'e ' b'ch']] -> [[b'e ' b'af' b'te' b'rn' b'oo' b'n ' b'th' b'e ' b'ch' b'ri']]\n",
      "[[b'th' b'e ' b'ch' b'ri' b'st' b'-c' b'hi' b'ld' b' h' b'ad']] -> [[b'e ' b'ch' b'ri' b'st' b'-c' b'hi' b'ld' b' h' b'ad' b' l']]\n",
      "[[b'ld' b' h' b'ad' b' l' b'ai' b'd ' b'hi' b'ms' b'el' b'f ']] -> [[b' h' b'ad' b' l' b'ai' b'd ' b'hi' b'ms' b'el' b'f ' b'in']]\n",
      "[[b'ms' b'el' b'f ' b'in' b' h' b'is' b' c' b'ra' b'dl' b'e-']] -> [[b'el' b'f ' b'in' b' h' b'is' b' c' b'ra' b'dl' b'e-' b'be']]\n",
      "[[b'ra' b'dl' b'e-' b'be' b'd ' b'an' b'd ' b'ha' b'd ' b'fa']] -> [[b'dl' b'e-' b'be' b'd ' b'an' b'd ' b'ha' b'd ' b'fa' b'll']]\n"
     ]
    }
   ],
   "source": [
    "ds = generate_tf_dataset(train_filenames, 2, window_size=10, batch_size=1).take(5)\n",
    "\n",
    "for record in ds:\n",
    "    print(record[0].numpy(), '->', record[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4nCJoKdP3Jj"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OUbQa0vgo6Xn"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnfSWUojP2dW"
   },
   "source": [
    "# Defining a TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5p95-2F5o8Js"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:340: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(name=name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "\n",
    "# The vectorization layer that will convert string bigrams to IDs\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=n_vocab, standardize=None,\n",
    "    split=None, input_shape=(window_size,)\n",
    ")\n",
    "\n",
    "# Train the model on existing data\n",
    "text_vectorizer.adapt(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnja2r0po-KD",
    "outputId": "0652f3ec-e8c4-414f-b7bd-959ceb0bff97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'e ', 'he', ' t', 'th', 'd ', ' a', ', ', ' h']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a few bigrams learnt by the vectorization layer\n",
    "text_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWDWqnh4QSoz"
   },
   "source": [
    "# Convert the targets from string ngrams to ngram IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9GPsI--vpAJJ"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x, y: (x, text_vectorizer(y)))\n",
    "valid_ds = valid_ds.map(lambda x, y: (x, text_vectorizer(y)))\n",
    "test_ds = test_ds.map(lambda x, y: (x, text_vectorizer(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WWOKBVFQa7b"
   },
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7S3tZlAHpBy5"
   },
   "outputs": [],
   "source": [
    "lm_model = models.Sequential([\n",
    "    text_vectorizer,\n",
    "    layers.Embedding(n_vocab+2, 96),\n",
    "    layers.LSTM(512, return_sequences=True),\n",
    "    layers.LSTM(256, return_sequences=True),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_vocab, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtH6Scd3Nq0D"
   },
   "source": [
    "# Metrics and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3EU4umkPqYGn"
   },
   "outputs": [],
   "source": [
    "# Inspired by https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "\n",
    "      # The next 4 lines zero-out the padding from loss calculations,\n",
    "      # this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics\n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "\n",
    "      # Calculating the perplexity steps:\n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      perplexity = K.exp(step1)\n",
    "\n",
    "      return perplexity\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      # Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations\n",
    "      super().update_state(perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BJlVhNmUqYGn"
   },
   "outputs": [],
   "source": [
    "lm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wa_7XIuSWVx",
    "outputId": "ead64876-1339-4389-9de1-67a4a89b6179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_1/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/events.py\", line 84, in _run\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_4648/2723294643.py\", line 1, in <module>\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 51, in train_step\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 901, in __call__\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 212, in call\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/functional.py\", line 167, in call\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/functional.py\", line 258, in _standardize_inputs\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/functional.py\", line 218, in _convert_inputs_to_tensors\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/ops/core.py\", line 822, in convert_to_tensor\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py\", line 132, in convert_to_tensor\n\nCast string to float is not supported\n\t [[{{node sequential_1/Cast}}]] [Op:__inference_one_step_on_iterator_45359]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_history \u001b[38;5;241m=\u001b[39m \u001b[43mlm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node sequential_1/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/asyncio/events.py\", line 84, in _run\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_4648/2723294643.py\", line 1, in <module>\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 51, in train_step\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 901, in __call__\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 212, in call\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/functional.py\", line 167, in call\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/functional.py\", line 258, in _standardize_inputs\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/models/functional.py\", line 218, in _convert_inputs_to_tensors\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/ops/core.py\", line 822, in convert_to_tensor\n\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py\", line 132, in convert_to_tensor\n\nCast string to float is not supported\n\t [[{{node sequential_1/Cast}}]] [Op:__inference_one_step_on_iterator_45359]"
     ]
    }
   ],
   "source": [
    "lstm_history = lm_model.fit(train_ds, validation_data=valid_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szqAC2qrSbfe",
    "outputId": "e0c27fb9-4d4b-4de3-fefb-0825aa288213"
   },
   "outputs": [],
   "source": [
    "lm_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdL853pgnYYX"
   },
   "source": [
    "# Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWVdBrOLmgi3"
   },
   "outputs": [],
   "source": [
    "# Define inputs to the model\n",
    "inp = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\n",
    "\n",
    "text_vectorized_out = lm_model.get_layer('text_vectorization')(inp)\n",
    "\n",
    "inp_state_c_lstm = tf.keras.layers.Input(shape=(512,))\n",
    "inp_state_h_lstm = tf.keras.layers.Input(shape=(512,))\n",
    "inp_state_c_lstm_1 = tf.keras.layers.Input(shape=(256,))\n",
    "inp_state_h_lstm_1 = tf.keras.layers.Input(shape=(256,))\n",
    "\n",
    "# Define embedding layer and output\n",
    "emb_layer = lm_model.get_layer('embedding')\n",
    "emb_out = emb_layer(text_vectorized_out)\n",
    "\n",
    "# Defining a LSTM layers and output\n",
    "lstm_layer = tf.keras.layers.LSTM(512, return_state=True, return_sequences=True)\n",
    "lstm_out, lstm_state_c, lstm_state_h = lstm_layer(emb_out, initial_state=[inp_state_c_lstm, inp_state_h_lstm])\n",
    "\n",
    "lstm_1_layer = tf.keras.layers.LSTM(256, return_state=True, return_sequences=True)\n",
    "lstm_1_out, lstm_1_state_c, lstm_1_state_h = lstm_1_layer(lstm_out, initial_state=[inp_state_c_lstm_1, inp_state_h_lstm_1])\n",
    "\n",
    "# Defining a Dense layer and output\n",
    "dense_out = lm_model.get_layer('dense')(lstm_1_out)\n",
    "\n",
    "# Defining the final Dense layer and output\n",
    "final_out = lm_model.get_layer('dense_1')(dense_out)\n",
    "#softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)\n",
    "\n",
    "# Copy the weights from the original model\n",
    "lstm_layer.set_weights(lm_model.get_layer('lstm').get_weights())\n",
    "lstm_1_layer.set_weights(lm_model.get_layer('lstm_1').get_weights())\n",
    "\n",
    "# Define final model\n",
    "infer_model = tf.keras.models.Model(\n",
    "    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, inp_state_c_lstm_1, inp_state_h_lstm_1],\n",
    "    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, lstm_1_state_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxc8UmDdwygh"
   },
   "source": [
    "# LSTM with Beam-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfLPQtvyw1_-"
   },
   "outputs": [],
   "source": [
    "def beam_one_step(model, input_, states):\n",
    "    \"\"\" Perform the model update and output for one step\"\"\"\n",
    "    out = model.predict([input_, *states])\n",
    "    output, new_states = out[0], out[1:]\n",
    "    return output, new_states\n",
    "\n",
    "\n",
    "def beam_search(model, input_, states, beam_depth=5, beam_width=3):\n",
    "    \"\"\" Defines an outer wrapper for the computational function of beam search \"\"\"\n",
    "\n",
    "    vocabulary = infer_model.get_layer(\"text_vectorization\").get_vocabulary()\n",
    "    index_word = dict(zip(range(len(vocabulary)), vocabulary))\n",
    "\n",
    "    def recursive_fn(input_, states, sequence, log_prob, i):\n",
    "        \"\"\" This function performs actual recursive computation of the long string\"\"\"\n",
    "\n",
    "        if i == beam_depth:\n",
    "            \"\"\" Base case: Terminate the beam search \"\"\"\n",
    "            results.append((list(sequence), states, np.exp(log_prob)))\n",
    "            return sequence, log_prob, states\n",
    "        else:\n",
    "            \"\"\" Recursive case: Keep computing the output using the previous outputs\"\"\"\n",
    "            output, new_states = beam_one_step(model, input_, states)\n",
    "\n",
    "            # Get the top beam_widht candidates for the given depth\n",
    "            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
    "            top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
    "\n",
    "            # For each candidate compute the next prediction\n",
    "            for p, wid in zip(top_probs, top_ids):\n",
    "                new_log_prob = log_prob + np.log(p)\n",
    "\n",
    "                # we are going to penalize joint probability whenever the same symbol is repeating\n",
    "                if len(sequence)>0 and wid == sequence[-1]:\n",
    "                    new_log_prob = new_log_prob + np.log(1e-1)\n",
    "\n",
    "                sequence.append(wid)\n",
    "                _ = recursive_fn(np.array([[index_word[wid]]]), new_states, sequence, new_log_prob, i+1)\n",
    "                sequence.pop()\n",
    "\n",
    "\n",
    "    results = []\n",
    "    sequence = []\n",
    "    log_prob = 0.0\n",
    "    recursive_fn(input_, states, sequence, log_prob, 0)\n",
    "\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nwbOs1Lw1__",
    "outputId": "86f55a19-fdba-4b62-8f42-804a1883923f"
   },
   "outputs": [],
   "source": [
    "text = [\"When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren ground\"]\n",
    "\n",
    "vocabulary = infer_model.get_layer(\"text_vectorization\").get_vocabulary()\n",
    "index_word = dict(zip(range(len(vocabulary)), vocabulary))\n",
    "\n",
    "\n",
    "seq = [text[0][i:i+2] for i in range(0, len(text[0]), 2)]\n",
    "\n",
    "# build up model state using the given string\n",
    "print(f\"Making {len(seq)} predictions from input\")\n",
    "\n",
    "\n",
    "# Reset the state of the model initially\n",
    "lm_model.reset_states()\n",
    "\n",
    "# Definin the initial state as all zeros\n",
    "state_c = np.zeros(shape=(1,512))\n",
    "state_h = np.zeros(shape=(1,512))\n",
    "state_c_1 = np.zeros(shape=(1,256))\n",
    "state_h_1 = np.zeros(shape=(1,256))\n",
    "\n",
    "states = [state_c, state_h, state_c_1, state_h_1]\n",
    "\n",
    "# Recursively update the model by assining new state to state\n",
    "for c in seq:\n",
    "    out, state_c, state_h, state_c_1, state_h_1 = infer_model.predict(\n",
    "        [np.array([[c]]), state_c, state_h, state_c_1, state_h_1]\n",
    ")\n",
    "\n",
    "# Get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "# Define first input to generate text recursively from\n",
    "x = np.array([[word]])\n",
    "\n",
    "# Predict for 100 time steps\n",
    "for i in range(50):\n",
    "    print('.', end='')\n",
    "\n",
    "    # Get the results from beam search\n",
    "    result = beam_search(infer_model, x, states, 5, 5)\n",
    "\n",
    "    # Get one of the top 10 results based on their likelihood\n",
    "    n_probs = np.array([p for _,_,p in result[:10]])\n",
    "    p_j = np.random.choice(list(range(n_probs.size)), p=n_probs/n_probs.sum())\n",
    "    best_beam_ids, states, _ = result[p_j]\n",
    "    x = np.array([[index_word[best_beam_ids[-1]]]])\n",
    "\n",
    "    text.extend([index_word[w] for w in best_beam_ids])\n",
    "\n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsFNiEKcPy0T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
