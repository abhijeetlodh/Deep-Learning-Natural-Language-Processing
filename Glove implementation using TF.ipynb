{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5de6QC8Fdvvb"
      },
      "outputs": [],
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "%matplotlib inline\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from matplotlib import pylab\n",
        "from scipy.sparse import lil_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEQqFSnGffWT"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGzvitiWedxa",
        "outputId": "c90c62c2-0a59-4ca6-f789-b163fe7718bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists\n",
            "bbc-fulltext.zip has already been extracted\n"
          ]
        }
      ],
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "\n",
        "\n",
        "def download_data(url, data_dir):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "\n",
        "    # Create the data directory if it does not exist\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "    # If file doesnt exist, download\n",
        "    if not os.path.exists(file_path):\n",
        "        print('Downloading file...')\n",
        "        filename, _ = urlretrieve(url, file_path)\n",
        "    else:\n",
        "        print(\"File already exists\")\n",
        "\n",
        "    extract_path = os.path.join(data_dir, 'bbc')\n",
        "\n",
        "    # If data has not been extracted already, extract data\n",
        "    if not os.path.exists(extract_path):\n",
        "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
        "            zipf.extractall(data_dir)\n",
        "    else:\n",
        "        print(\"bbc-fulltext.zip has already been extracted\")\n",
        "\n",
        "download_data(url, 'data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muFh9CAXfgRH"
      },
      "source": [
        "# Reading data without preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgQmRuhekPN",
        "outputId": "b6d4e33e-eeae-46b1-dd16-4d4f02f3017e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 125.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Millions to miss out on the net  By 2025, 40% of t\n",
            "Example words (end):  ices rose 43% during the year, with copper up 36%.\n"
          ]
        }
      ],
      "source": [
        "def read_data(data_dir):\n",
        "\n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "\n",
        "    print(\"Reading files\")\n",
        "\n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "\n",
        "        for fi, f in enumerate(files):\n",
        "\n",
        "            # We don't read the README file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "\n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "\n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "\n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "\n",
        "                    story.append(row.strip())\n",
        "\n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)\n",
        "                # Add that to the list\n",
        "                news_stories.append(story)\n",
        "\n",
        "        print('', end='\\r')\n",
        "\n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories\n",
        "\n",
        "\n",
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSuLi1b5fMAA"
      },
      "source": [
        "# Building a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3dPjLXteljG",
        "outputId": "073fd9a8-b273-499e-8324-585b5a3cbc44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "n_vocab = 15000 + 1\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab - 1,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', oov_token=''\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n98pv62m5UjV",
        "outputId": "dd9667a7-654e-44d1-80cb-967b499cdc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "It took 847.5881609916687 seconds to generate the co-occurrence matrix\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import save_npz, load_npz\n",
        "\n",
        "def generate_cooc_matrix(text, tokenizer, window_size, n_vocab, use_weighting=True):\n",
        "\n",
        "    # Convert list of text to list of list of word IDs\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    # A sparse matrix to retain co-occurrences of words\n",
        "    cooc_mat = lil_matrix((n_vocab, n_vocab), dtype=np.float32)\n",
        "\n",
        "    # Go through each sequence one by one\n",
        "    for si, sequence in enumerate(sequences):\n",
        "\n",
        "        # Printing the progress\n",
        "        if (si+1)%100==0:\n",
        "            print('.'*((si+1)//100), f\"{si+1}/{len(sequences)}\", end='\\r')\n",
        "\n",
        "        # For each target word,\n",
        "        for i, wi in zip(np.arange(window_size, len(sequence)-window_size), sequence[window_size:-window_size]):\n",
        "\n",
        "            # Get the context window word IDs\n",
        "            context_window = sequence[i-window_size: i+window_size+1]\n",
        "\n",
        "            # The weight for the words in the context window (except target word) will be 1\n",
        "            window_weights = np.ones(shape=(window_size*2 + 1,), dtype=np.float32)\n",
        "            window_weights[window_size] = 0.0\n",
        "\n",
        "            if use_weighting:\n",
        "                # If weighting is used, penalize context words based on distance to target word\n",
        "                distances = np.abs(np.arange(-window_size, window_size+1))\n",
        "                distances[window_size] = 1.0\n",
        "                # Update the sparse matrix\n",
        "                cooc_mat[wi, context_window] += window_weights/distances\n",
        "            else:\n",
        "                # Update the sparse matrix\n",
        "                cooc_mat[wi, context_window] += window_weights\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return cooc_mat\n",
        "\n",
        "# ----------------------------------------- IMPORTANT ---------------------------------------------- #\n",
        "#                                                                                                    #\n",
        "# Set this true or false, depending on whether you want to generate the matrix or reuse the existing #\n",
        "#                                                                                                    #\n",
        "# ---------------------------------------------------------------------------------------------------#\n",
        "generate_cooc = True\n",
        "\n",
        "# Generate the matrix\n",
        "if generate_cooc:\n",
        "    t1 = time.time()\n",
        "    cooc_mat = generate_cooc_matrix(news_stories, tokenizer, 1, n_vocab, True)\n",
        "    t2 = time.time()\n",
        "    print(f\"It took {t2-t1} seconds to generate the co-occurrence matrix\")\n",
        "\n",
        "    save_npz(os.path.join('data','cooc_mat.npz'), cooc_mat.tocsr())\n",
        "# Load the matrix from disk\n",
        "else:\n",
        "    try:\n",
        "        cooc_mat = load_npz(os.path.join('data','cooc_mat.npz')).tolil()\n",
        "        print(f\"Cooc matrix of type {type(cooc_mat).__name__} was loaded from disk\")\n",
        "    except FileNotFoundError as ex:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not find the co-occurrence matrix on the disk. Did you generate the matrix by setting generate_cooc=True?\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X39s1WqkfC3a"
      },
      "source": [
        "# Defining hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JpqudHOdeo4I"
      },
      "outputs": [],
      "source": [
        "batch_size = 4096 # Data points in a single batch\n",
        "\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "window_size=1 # We use a window size of 1 on either side of target word\n",
        "\n",
        "epochs = 5 # Number of epochs to train for\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors\n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid data points randomly from a large window without always being deterministic\n",
        "valid_window = 250\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as\n",
        "# some moderately rare words\n",
        "np.random.seed(54321)\n",
        "random.seed(54321)\n",
        "\n",
        "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_term_ids = np.append(\n",
        "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
        "    axis=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z8W3r0ffEHU"
      },
      "source": [
        "# Defining the model computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyZXmaa1esbN",
        "outputId": "88eb0ec8-6896-4db8-9cdd-3ed1481e3028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"glove_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " target_embedding (Embeddin  (None, 128)                  1920128   ['input_1[0][0]']             \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " context_embedding (Embeddi  (None, 128)                  1920128   ['input_2[0][0]']             \n",
            " ng)                                                                                              \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 1)                    0         ['target_embedding[0][0]',    \n",
            "                                                                     'context_embedding[0][0]']   \n",
            "                                                                                                  \n",
            " target_embedding_bias (Emb  (None, 1)                    15001     ['input_1[0][0]']             \n",
            " edding)                                                                                          \n",
            "                                                                                                  \n",
            " context_embedding_bias (Em  (None, 1)                    15001     ['input_2[0][0]']             \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 1)                    0         ['dot[0][0]',                 \n",
            "                                                                     'target_embedding_bias[0][0]'\n",
            "                                                                    , 'context_embedding_bias[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3870258 (14.76 MB)\n",
            "Trainable params: 3870258 (14.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Input, Embedding, Dot, Add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Define two input layers for context and target words\n",
        "word_i = Input(shape=())\n",
        "word_j = Input(shape=())\n",
        "\n",
        "# Each context and target has their own embeddings (weights and biases)\n",
        "\n",
        "# Embedding weights\n",
        "embeddings_i = Embedding(n_vocab, embedding_size, name='target_embedding')(word_i)\n",
        "embeddings_j = Embedding(n_vocab, embedding_size, name='context_embedding')(word_j)\n",
        "\n",
        "# Embedding biases\n",
        "b_i = Embedding(n_vocab, 1, name='target_embedding_bias')(word_i)\n",
        "b_j = Embedding(n_vocab, 1, name='context_embedding_bias')(word_j)\n",
        "\n",
        "# Compute the dot product between embedding vectors (i.e., w_i.w_j)\n",
        "ij_dot = Dot(axes=-1)([embeddings_i,embeddings_j])\n",
        "\n",
        "# Add the biases (i.e., w_i.w_j + b_i + b_j )\n",
        "pred = Add()([ij_dot, b_i, b_j])\n",
        "\n",
        "# The final model\n",
        "glove_model = Model(inputs=[word_i, word_j],outputs=pred, name='glove_model')\n",
        "\n",
        "# Glove has a specific loss function with a sound mathematical underpinning\n",
        "# It is a form of mean squared error\n",
        "glove_model.compile(loss=\"mse\", optimizer = 'adam')\n",
        "\n",
        "glove_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7B0zJNHup8S6"
      },
      "outputs": [],
      "source": [
        "news_sequences = tokenizer.texts_to_sequences(news_stories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbGt8bbMpvnr",
        "outputId": "0fad0ee4-f3d0-4d71-a8c6-a37c299cc899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([ 1955,   976,  1490,  1953,  3957, 13080,  2315,  3149,   123,\n",
            "       13666]), array([   4,    5,   33,  983,    5,   46,   51, 1953, 1335, 3717]))\n",
            "[1.9459102 1.609438  1.9459102 0.        1.7917595 0.        0.\n",
            " 0.6931472 0.6931472 0.6931472]\n",
            "[0.12123093 0.08944272 0.12123093 0.         0.10573713 0.\n",
            " 0.         0.03162277 0.03162277 0.03162277]\n"
          ]
        }
      ],
      "source": [
        "def glove_data_generator(\n",
        "    sequences, window_size, batch_size, vocab_size, cooccurrence_matrix, x_max=100.0, alpha=0.75, seed=None\n",
        "):\n",
        "    \"\"\" Generate batches of inputs and targets for GloVe \"\"\"\n",
        "\n",
        "    # Shuffle the data so that in every epoch, the order of data is different.\n",
        "    rand_sequence_ids = np.arange(len(sequences))\n",
        "    np.random.shuffle(rand_sequence_ids)\n",
        "\n",
        "    # We will use a sampling table to make sure we don't oversample stop words\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "    # For each story/article\n",
        "    for si in rand_sequence_ids:\n",
        "\n",
        "        # Generate positive skip-grams while using subsampling\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequences[si],\n",
        "            vocabulary_size=vocab_size,\n",
        "            window_size=window_size,\n",
        "            negative_samples=0.0,\n",
        "            shuffle=False,\n",
        "            sampling_table=sampling_table,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Take targets and context words separately\n",
        "        targets, context = zip(*positive_skip_grams)\n",
        "        targets, context = np.array(targets).ravel(), np.array(context).ravel()\n",
        "\n",
        "\n",
        "        x_ij = np.array(cooccurrence_matrix[targets, context].toarray()).ravel()\n",
        "\n",
        "        # Compute log - Introducing an additive shift to make sure we don't compute log(0)\n",
        "        log_x_ij = np.log(x_ij + 1)\n",
        "\n",
        "        # Sample weights\n",
        "        # if x < x_max => (x/x_max)**alpha / else => 1\n",
        "        sample_weights = np.where(x_ij < x_max, (x_ij/x_max)**alpha, 1)\n",
        "\n",
        "        # If seed is not provided, generate a random one\n",
        "        if not seed:\n",
        "            seed = random.randint(0, 10e6)\n",
        "\n",
        "        # Shuffle data\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(context)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(targets)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(log_x_ij)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(sample_weights)\n",
        "\n",
        "        # Generate a batch or data in the format\n",
        "        # ((target words, context words), log(X_ij) <- true targets, f(X_ij) <- sample weights)\n",
        "        for eg_id_start in range(0, context.shape[0], batch_size):\n",
        "            yield (\n",
        "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
        "                context[eg_id_start: min(eg_id_start+batch_size, context.shape[0])]\n",
        "            ), log_x_ij[eg_id_start: min(eg_id_start+batch_size, x_ij.shape[0])], \\\n",
        "            sample_weights[eg_id_start: min(eg_id_start+batch_size, sample_weights.shape[0])]\n",
        "\n",
        "\n",
        "# Generate some data\n",
        "news_glove_data_gen = glove_data_generator(\n",
        "    news_sequences, 2, 10, n_vocab, cooc_mat\n",
        ")\n",
        "\n",
        "for x, y, z in news_glove_data_gen:\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(z)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mOhaJiYsaqRh"
      },
      "outputs": [],
      "source": [
        "class ValidationCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
        "\n",
        "        self.valid_term_ids = valid_term_ids\n",
        "        self.model_with_embeddings = model_with_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\" Validation logic \"\"\"\n",
        "\n",
        "        # We will use context embeddings to get the most similar words\n",
        "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
        "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
        "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
        "\n",
        "        # Get the embeddings corresponding to valid_term_ids\n",
        "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
        "\n",
        "        # Compute the similarity between valid_term_ids and all the embeddings\n",
        "        # V x d (d x D) => V x D\n",
        "        top_k = 5 # Top k items will be displayed\n",
        "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
        "\n",
        "        # Invert similarity matrix to negative\n",
        "        # Ignore the first one because that would be the same word as the probe word\n",
        "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
        "\n",
        "        # Print the output\n",
        "        for i, term_id in enumerate(valid_term_ids):\n",
        "\n",
        "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j > 1])\n",
        "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
        "\n",
        "        print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dznUt-rnaqlS",
        "outputId": "f5deb450-8bc3-416c-881b-3105534a408a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/5 started\n",
            "   2224/Unknown - 121s 54ms/step - loss: 0.5948election: attorney, forthcoming, motors, trail, director\n",
            "me: many, network, any, technology, just\n",
            "with: for, strong, behind, great, begin\n",
            "you: they, we, it, still, now\n",
            "were: are, was, if, could, did\n",
            "win: way, some, its, time, games\n",
            "those: people, made, say, won, came\n",
            "music: three, 30, media, 16, games\n",
            "also: already, made, who, been, still\n",
            "around: behind, now, set, strong, made\n",
            "best: supporting, actress, category, actor, its\n",
            "him: whether, made, but, people, reports\n",
            "too: how, very, so, pretty, stronger\n",
            "into: through, back, them, strong, under\n",
            "through: behind, strong, now, back, power\n",
            "mr: gordon, tony, charles, mikhail, ken\n",
            "file: systems, intel, quoted, copyrighted, publication\n",
            "road: at, pretend, bowden, subdued, judgment\n",
            "ceremony: time, the, largest, way, sunday's\n",
            "mini: cross, stretch, time, his, wednesday\n",
            "unit: days, rules, tourist, games, minutes\n",
            "details: size, many, its, january, users\n",
            "code: da, ownership, apart, service, wireless\n",
            "indian: german, award, changing, telecoms, island\n",
            "operating: financial, points, technology, show's, time\n",
            "actually: done, would, can, will, also\n",
            "deutsche: austria, boosts, austria's, energis, intervoice\n",
            "offering: up, under, against, showing, net\n",
            "machine: play, like, sec, that's, him\n",
            "profit: the, his, trade, thursday, time\n",
            "stay: her, way, series, films\n",
            "significant: of, different, showing, taking, number\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 121s 54ms/step - loss: 0.5945\n",
            "Epoch: 2/5 started\n",
            "   2225/Unknown - 121s 55ms/step - loss: 0.0370election: forthcoming, attorney, labour's, conservatives', trail\n",
            "me: place, looking, fall, order, network\n",
            "with: constructive, honoured, together, bbc's, reads\n",
            "you: we, they, know, asked, exactly\n",
            "were: are, was, need, easier, because\n",
            "win: way, victory, return, move, form\n",
            "those: used, took, came, won, made\n",
            "music: mp3, cameras, divide, media, processes\n",
            "also: already, made, asked, never, play\n",
            "around: spent, comments, who's, deal, alternative\n",
            "best: supporting, category, actress, counterparts, fall\n",
            "him: met, but, went, reports, means\n",
            "too: how, pretty, so, very, worse\n",
            "into: hutton, driving, through, alan's, fatal\n",
            "through: strong, helps, who's, behind, edit\n",
            "mr: 63, bernie, resignation, closest, ken\n",
            "file: illegally, systems, baby's, quoted, sweden\n",
            "road: at, pretend, subdued, judgment, forwards\n",
            "ceremony: brit, grammy, least, debate, present\n",
            "mini: gates, cross, period, time, round\n",
            "unit: minutes, putting, debut, sort, continued\n",
            "details: return, popularity, compatible, means, result\n",
            "code: da, aviation, disillusioned, upload, troubled\n",
            "indian: recognition, award, toughest, lords, german\n",
            "operating: financial, labelling, went, points, ces\n",
            "actually: must, they've, will, may, done\n",
            "deutsche: austria, energis, austria's, boosts, cocoa\n",
            "offering: net, 14, saw, expects, gets\n",
            "machine: deacon, nitoglia, him, capable, ensure\n",
            "profit: the, period, europe's, time, wreck\n",
            "stay: use, children, looking, return, result\n",
            "significant: of, distributed, fundamental, minutes, houlihan\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 122s 55ms/step - loss: 0.0370\n",
            "Epoch: 3/5 started\n",
            "   2224/Unknown - 119s 53ms/step - loss: 0.0158election: attorney, forthcoming, labour's, conservatives', november's\n",
            "me: aimed, chosen, order, october, gathered\n",
            "with: bruising, wham, uphill, together, sing\n",
            "you: they, we, god, asked, nobody\n",
            "were: are, was, punished, easier, when\n",
            "win: victory, 60, focus, directed, terms\n",
            "those: fear, meant, someone, dealer, used\n",
            "music: cameras, mp3, subscriber, media, revolution\n",
            "also: already, afford, play, remain, allow\n",
            "around: comments, signed, sign, who's, alternative\n",
            "best: supporting, counterparts, actress, category, query\n",
            "him: look, comment, then, went, used\n",
            "too: how, larger, so, pretty, very\n",
            "into: hutton, fatal, driving, through, rat\n",
            "through: spend, suggested, dancing, fatal, door\n",
            "mr: bernie, 63, closest, resignation, malcolm\n",
            "file: systems, illegally, baby's, quoted, passion\n",
            "road: at, pretend, revenge, dulko, subdued\n",
            "ceremony: brit, grammy, least, present, analyst\n",
            "mini: independence, gates, cross, period, ocean\n",
            "unit: adverse, minutes, bench, putting, housing\n",
            "details: passwords, grab, result, india, fire\n",
            "code: da, disillusioned, ears, pedro, ownership\n",
            "indian: relationships, sell, patents, snap, avoid\n",
            "operating: went, labelling, technology, liberty, bsa\n",
            "actually: on, they've, i've, directly, languages\n",
            "deutsche: austria, austria's, energis, central, cocoa\n",
            "offering: 30, claiming, surpassed, subscriber, largely\n",
            "machine: nitoglia, deacon, cullen, moody, gene\n",
            "profit: volvo, vacant, thursday, table, period\n",
            "stay: return, trying, failing, keen, appeal\n",
            "significant: of, minutes, pull, carried, instrumental\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 119s 53ms/step - loss: 0.0158\n",
            "Epoch: 4/5 started\n",
            "   2225/Unknown - 113s 51ms/step - loss: 0.0115election: attorney, forthcoming, labour's, conservatives', commitments\n",
            "me: example, aimed, optimistic, uwb, october\n",
            "with: reservoir, sing, accession, bruising, supposed\n",
            "you: they, we, afford, asked, god\n",
            "were: are, easier, was, punished, shall\n",
            "win: victory, failing, support, planning, 55s\n",
            "those: starred, registrars, fear, women, someone\n",
            "music: cameras, mp3, unlimited, refuseniks, media\n",
            "also: already, allow, going, play, give\n",
            "around: comments, build, signed, government's, lot\n",
            "best: supporting, category, counterparts, actress, superman\n",
            "him: look, then, compete, comment, seeks\n",
            "too: pretty, how, phenomenally, larger, so\n",
            "into: fatal, hutton, sec's, rat, through\n",
            "through: spend, door, fatal, to, dancing\n",
            "mr: bernie, 63, malcolm, resignation, gordon\n",
            "file: systems, illegally, baby's, quoted, atlantic\n",
            "road: at, dulko, sailing, subdued, pretend\n",
            "ceremony: brit, least, grammy, gates, topic\n",
            "mini: gates, independence, ocean, constitution, friday\n",
            "unit: rapid, gameboy, bench, adverse, families\n",
            "details: passwords, grab, prove, fire, altria\n",
            "code: da, ownership, easymobile, ears, pen\n",
            "indian: wants, pivotal, sues, sell, downloading\n",
            "operating: bsa, went, labelling, financial, sceptic\n",
            "actually: on, languages, bust, accept, gibraltar\n",
            "deutsche: austria, austria's, energis, central, intervoice\n",
            "offering: precinct, vary, largely, 30m, linux\n",
            "machine: nitoglia, deacon, masi, cullen, fortunate\n",
            "profit: volvo, lords, logs, vacant, independence\n",
            "stay: trying, chance, grow, appeal, according\n",
            "significant: ruled, carried, pulled, of, edged\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 113s 51ms/step - loss: 0.0115\n",
            "Epoch: 5/5 started\n",
            "   2225/Unknown - 116s 52ms/step - loss: 0.0094election: attorney, labour's, posters, commitments, guides\n",
            "me: example, aimed, optimistic, write, lay\n",
            "with: reservoir, gizmondo, accession, sing, supposed\n",
            "you: they, afford, we, asked, goodness\n",
            "were: are, shall, easier, will, was\n",
            "win: victory, failing, support, 55s, planning\n",
            "those: dealer, somebody, registrars, fear, paver\n",
            "music: cameras, unlimited, mp3, refuseniks, revolution\n",
            "also: announced, yet, continue, already, allow\n",
            "around: comments, whole, government's, conservatives, company's\n",
            "best: supporting, counterparts, actress, superman, category\n",
            "him: sorry, driven, annually, ensure, used\n",
            "too: pretty, interference, larger, how, bigger\n",
            "into: hutton, sec's, fatal, rat, through\n",
            "through: door, row, fatal, rigging, edit\n",
            "mr: 63, bernie, resignation, gordon, tony\n",
            "file: systems, illegally, quoted, baby's, adding\n",
            "road: at, subdued, sailing, variant, dulko\n",
            "ceremony: brit, grammy, independence, wreck, vacant\n",
            "mini: independence, gates, ocean, month's, friday\n",
            "unit: crop, families, crafted, alike, adverse\n",
            "details: passwords, nervous, grab, determination, figure\n",
            "code: da, easymobile, ears, disillusioned, pen\n",
            "indian: waited, wants, disgrace, pin, visit\n",
            "operating: develops, bsa, justice, sensing, technology\n",
            "actually: on, bust, tighter, night's, accept\n",
            "deutsche: austria, austria's, energis, central, intervoice\n",
            "offering: vary, puzzle, linux, precinct, fahrenheit\n",
            "machine: nitoglia, deacon, cullen, moody, fortunate\n",
            "profit: volvo, lords, cafes, zurich, blessed\n",
            "stay: maintain, trying, respond, designed, according\n",
            "significant: ruled, carried, of, 70th, integral\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 116s 52ms/step - loss: 0.0094\n"
          ]
        }
      ],
      "source": [
        "glove_validation_callback = ValidationCallback(valid_term_ids, glove_model, tokenizer)\n",
        "\n",
        "# Train the model for several epochs\n",
        "for ei in range(epochs):\n",
        "\n",
        "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
        "\n",
        "    news_glove_data_gen = glove_data_generator(\n",
        "        news_sequences, window_size, batch_size, n_vocab, cooc_mat\n",
        "    )\n",
        "\n",
        "    glove_model.fit(\n",
        "        news_glove_data_gen, epochs=1,\n",
        "        callbacks=glove_validation_callback,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IHTyweeSb-NI"
      },
      "outputs": [],
      "source": [
        "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Get the words sorted according to their ID from the tokenizer\n",
        "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
        "    # Add one word in front to represent the reserved ID (0)\n",
        "    words_sorted = [None] + list(words_sorted)\n",
        "\n",
        "    # Create a new array by concatenating embeddings and bias\n",
        "\n",
        "    context_embedding_weights = model.get_layer(\"context_embedding\").get_weights()[0]\n",
        "    context_embedding_bias = model.get_layer(\"context_embedding_bias\").get_weights()[0]\n",
        "    context_embedding = np.concatenate([context_embedding_weights, context_embedding_bias], axis=1)\n",
        "\n",
        "    target_embedding_weights = model.get_layer(\"target_embedding\").get_weights()[0]\n",
        "    target_embedding_bias = model.get_layer(\"target_embedding_bias\").get_weights()[0]\n",
        "    target_embedding = np.concatenate([target_embedding_weights, target_embedding_bias], axis=1)\n",
        "\n",
        "    # Save the array as pandas DataFrames\n",
        "    pd.DataFrame(\n",
        "        context_embedding,\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"context_embedding_and_bias.pkl\"))\n",
        "\n",
        "    pd.DataFrame(\n",
        "        target_embedding,\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"target_embedding_and_bias.pkl\"))\n",
        "\n",
        "\n",
        "save_embeddings(glove_model, tokenizer, n_vocab, save_dir='glove_embeddings')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
