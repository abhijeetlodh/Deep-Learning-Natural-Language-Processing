{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QxHMmGBh-o7O"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them before proceeding further.\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjWS6aFxG2JL",
    "outputId": "11a8ed03-7287-44ed-aaf7-bc3bfbfae62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "\n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "\n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1AJpLd6oG2zB",
    "outputId": "5b28fc54-6687-43c6-9626-bf0e0cfaabb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 387.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Martinez sees off Vinci challenge  Veteran Spaniar\n",
      "Example words (end):  e the day - but it is widely expected to be 5 May.\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "\n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "\n",
    "    print(\"Reading files\")\n",
    "\n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "\n",
    "        for fi, f in enumerate(files):\n",
    "\n",
    "            # We don't read the README file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "\n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "\n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "\n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "\n",
    "                    story.append(row.strip())\n",
    "\n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)\n",
    "                # Add that to the list\n",
    "                news_stories.append(story)\n",
    "\n",
    "        print('', end='\\r')\n",
    "\n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "\n",
    "\n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9ZA7pRFG6wJ",
    "outputId": "3536bd00-e5ef-413b-eb4e-d60fd7abb6c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDVGZDLkG7Lw",
    "outputId": "5fadbd80-f30a-4632-d6af-1c4934467c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32360\n",
      "\n",
      "Words at the top\n",
      "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
      "\n",
      "Words at the bottom\n",
      "\t {'â£102': 32350, 'congratulates': 32351, 'anew': 32352, \"kerry's\": 32353, 'warmest': 32354, 'umbilical': 32355, 'myron': 32356, 'ebell': 32357, 'presume': 32358, 'â£33': 32359}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vdU7rzfG8-4",
    "outputId": "d1e0f94f-afd1-4412-94bf-dada27d2d2fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab-1,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', oov_token='',\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KWE0cskXG_I4"
   },
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g9pUPKq4HYLy"
   },
   "outputs": [],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jH5MCTg9c_3",
    "outputId": "c1eda13a-080b-4135-e21f-f631cca05973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28, 428, 132, 3, 2, 1500]]\n",
      "Input: (696, [28, 132]) ([['i', 'going'], 'age'])/ Label: 0\n",
      "Input: (3, [132, 2]) ([['going', 'the'], 'to'])/ Label: 1\n",
      "Input: (2, [132, 2]) ([['going', 'the'], 'the'])/ Label: 0\n",
      "Input: (1395, [132, 2]) ([['going', 'the'], 'appeared'])/ Label: 0\n",
      "Input: (3, [132, 2]) ([['going', 'the'], 'to'])/ Label: 0\n",
      "Input: (9735, [28, 132]) ([['i', 'going'], \"father's\"])/ Label: 0\n",
      "Input: (64, [428, 3]) ([['am', 'to'], 'first'])/ Label: 0\n",
      "Input: (47, [3, 1500]) ([['to', 'store'], 'new'])/ Label: 0\n",
      "Input: (3257, [3, 1500]) ([['to', 'store'], 'dogs'])/ Label: 0\n",
      "Input: (2, [428, 3]) ([['am', 'to'], 'the'])/ Label: 0\n",
      "Input: (428, [28, 132]) ([['i', 'going'], 'am'])/ Label: 1\n",
      "Input: (2, [3, 1500]) ([['to', 'store'], 'the'])/ Label: 1\n",
      "Input: (329, [3, 1500]) ([['to', 'store'], 'match'])/ Label: 0\n",
      "Input: (14, [28, 132]) ([['i', 'going'], 'was'])/ Label: 0\n",
      "Input: (132, [428, 3]) ([['am', 'to'], 'going'])/ Label: 1\n",
      "Input: (0, [132, 2]) ([['going', 'the'], None])/ Label: 0\n",
      "Input: (8, [428, 3]) ([['am', 'to'], 'for'])/ Label: 0\n",
      "Input: (140, [428, 3]) ([['am', 'to'], '6'])/ Label: 0\n",
      "Input: (328, [3, 1500]) ([['to', 'store'], 'times'])/ Label: 0\n",
      "Input: (74, [28, 132]) ([['i', 'going'], 'she'])/ Label: 0\n"
     ]
    }
   ],
   "source": [
    "def cbow_grams(sequence, vocabulary_size,\n",
    "              window_size=4, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    for i, wi in enumerate(sequence):\n",
    "\n",
    "\n",
    "        if not wi or i < window_size or i + 1 > len(sequence)-window_size:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "\n",
    "        context_words = [wj for j, wj in enumerate(sequence[window_start:window_end]) if j+window_start != i]\n",
    "        target_word = wi\n",
    "\n",
    "        context_classes = tf.expand_dims(tf.constant(context_words, dtype=\"int64\"), 0)\n",
    "\n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_classes,\n",
    "          num_true=window_size * 2,\n",
    "          num_sampled=negative_samples,\n",
    "          unique=True,\n",
    "          range_max=vocabulary_size,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "        # Build context and label vectors (for one target word)\n",
    "        negative_targets = negative_sampling_candidates.numpy().tolist()\n",
    "\n",
    "        target = [target_word] + negative_targets\n",
    "        label = [1] + [0]*negative_samples\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.extend(target)\n",
    "        contexts.extend([context_words]*(negative_samples+1))\n",
    "        labels.extend(label)\n",
    "\n",
    "    couples = list(zip(targets, contexts))\n",
    "\n",
    "    seed = random.randint(0, 10e6)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(couples)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(labels)\n",
    "\n",
    "\n",
    "    return couples, labels\n",
    "\n",
    "\n",
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "\n",
    "inputs, labels = cbow_grams(\n",
    "    tokenizer.texts_to_sequences([\"I am going to the store\"])[0],\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size, negative_samples=4, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "print(tokenizer.texts_to_sequences([\"I am going to the store\"]))\n",
    "i = 0\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    i += 1\n",
    "    print(f\"Input: {inp} ({[[tokenizer.index_word[wi] for wi in inp[1] ]] + [tokenizer.index_word[inp[0]] if inp[0] > 0 else None]})/ Label: {lbl}\")\n",
    "    #\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3YWBLAw-6K1",
    "outputId": "137e1468-64c2-4c0d-f125-9f228ff82006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [5092, 1784] (['martinez', 'sees']) / Label: 1\n",
      "Input: [1784, 5092] (['sees', 'martinez']) / Label: 1\n",
      "Input: [1784, 127] (['sees', 'off']) / Label: 1\n",
      "Input: [127, 1784] (['off', 'sees']) / Label: 1\n",
      "Input: [127, 4366] (['off', 'vinci']) / Label: 1\n",
      "Input: [4366, 127] (['vinci', 'off']) / Label: 1\n",
      "Input: [4366, 945] (['vinci', 'challenge']) / Label: 1\n",
      "Input: [945, 4366] (['challenge', 'vinci']) / Label: 1\n",
      "Input: [945, 2178] (['challenge', 'veteran']) / Label: 1\n",
      "Input: [2178, 945] (['veteran', 'challenge']) / Label: 1\n",
      "Input: [2178, 3826] (['veteran', 'spaniard']) / Label: 1\n",
      "Input: [3826, 2178] (['spaniard', 'veteran']) / Label: 1\n",
      "Input: [3826, 9001] (['spaniard', 'conchita']) / Label: 1\n",
      "Input: [9001, 3826] (['conchita', 'spaniard']) / Label: 1\n",
      "Input: [9001, 5092] (['conchita', 'martinez']) / Label: 1\n",
      "Input: [5092, 9001] (['martinez', 'conchita']) / Label: 1\n",
      "Input: [5092, 267] (['martinez', 'came']) / Label: 1\n",
      "Input: [267, 5092] (['came', 'martinez']) / Label: 1\n",
      "Input: [267, 26] (['came', 'from']) / Label: 1\n",
      "Input: [26, 267] (['from', 'came']) / Label: 1\n",
      "Input: [26, 6] (['from', 'a']) / Label: 1\n",
      "Input: [6, 26] (['a', 'from']) / Label: 1\n",
      "Input: [6, 110] (['a', 'set']) / Label: 1\n",
      "Input: [110, 6] (['set', 'a']) / Label: 1\n",
      "Input: [110, 130] (['set', 'down']) / Label: 1\n",
      "Input: [130, 110] (['down', 'set']) / Label: 1\n",
      "Input: [130, 3] (['down', 'to']) / Label: 1\n",
      "Input: [3, 130] (['to', 'down']) / Label: 1\n",
      "Input: [3, 567] (['to', 'beat']) / Label: 1\n",
      "Input: [567, 3] (['beat', 'to']) / Label: 1\n",
      "Input: [567, 1729] (['beat', 'italian']) / Label: 1\n",
      "Input: [1729, 567] (['italian', 'beat']) / Label: 1\n",
      "Input: [1729, 1] (['italian', '']) / Label: 1\n",
      "Input: [1, 1729] (['', 'italian']) / Label: 1\n",
      "Input: [1, 4366] (['', 'vinci']) / Label: 1\n",
      "Input: [4366, 1] (['vinci', '']) / Label: 1\n",
      "Input: [4366, 21] (['vinci', 'at']) / Label: 1\n",
      "Input: [21, 4366] (['at', 'vinci']) / Label: 1\n",
      "Input: [21, 2] (['at', 'the']) / Label: 1\n",
      "Input: [2, 21] (['the', 'at']) / Label: 1\n",
      "Input: [2, 5093] (['the', 'qatar']) / Label: 1\n",
      "Input: [5093, 2] (['qatar', 'the']) / Label: 1\n",
      "Input: [5093, 250] (['qatar', 'open']) / Label: 1\n",
      "Input: [250, 5093] (['open', 'qatar']) / Label: 1\n",
      "Input: [250, 7] (['open', 'in']) / Label: 1\n",
      "Input: [7, 250] (['in', 'open']) / Label: 1\n",
      "Input: [7, 12988] (['in', 'doha']) / Label: 1\n",
      "Input: [12988, 7] (['doha', 'in']) / Label: 1\n",
      "Input: [12988, 1361] (['doha', 't']) / Label: 1\n",
      "Input: [1361, 12988] (['t', 'doha']) / Label: 1\n",
      "Input: [267, 10154] (['came', 'declarations']) / Label: 0\n",
      "Input: [250, 27221] (['open', 'trademarks']) / Label: 0\n",
      "Input: [127, 30133] (['off', 'personalized']) / Label: 0\n",
      "Input: [2178, 11380] (['veteran', 'fuming']) / Label: 0\n",
      "Input: [2, 27655] (['the', 'winkler']) / Label: 0\n",
      "Input: [250, 27793] (['open', 'storey']) / Label: 0\n",
      "Input: [130, 25924] (['down', 'redifining']) / Label: 0\n",
      "Input: [3826, 2056] (['spaniard', 'queen']) / Label: 0\n",
      "Input: [945, 21619] (['challenge', 'freshen']) / Label: 0\n",
      "Input: [4366, 821] (['vinci', 'gave']) / Label: 0\n",
      "Input: [110, 25577] (['set', \"dominica's\"]) / Label: 0\n",
      "Input: [945, 26793] (['challenge', 'matador']) / Label: 0\n",
      "Input: [1784, 4702] (['sees', 'sustain']) / Label: 0\n",
      "Input: [12988, 16563] (['doha', 'basel']) / Label: 0\n",
      "Input: [6, 4840] (['a', 'musician']) / Label: 0\n",
      "Input: [26, 21479] (['from', 'generoso']) / Label: 0\n",
      "Input: [5092, 19938] (['martinez', 'inhabitants']) / Label: 0\n",
      "Input: [1784, 22998] (['sees', 'reseach']) / Label: 0\n",
      "Input: [21, 11075] (['at', 'ransom']) / Label: 0\n",
      "Input: [9001, 3530] (['conchita', 'dangerous']) / Label: 0\n",
      "Input: [9001, 5987] (['conchita', 'bogdanovic']) / Label: 0\n",
      "Input: [5092, 5622] (['martinez', 'agrees']) / Label: 0\n",
      "Input: [4366, 26029] (['vinci', 'winstone']) / Label: 0\n",
      "Input: [3826, 16628] (['spaniard', 'berth']) / Label: 0\n",
      "Input: [7, 15627] (['in', 'effortlessly']) / Label: 0\n",
      "Input: [6, 9722] (['a', 'phantom']) / Label: 0\n",
      "Input: [2, 225] (['the', 'need']) / Label: 0\n",
      "Input: [21, 6147] (['at', 'honorary']) / Label: 0\n",
      "Input: [1729, 11292] (['italian', 'stradey']) / Label: 0\n",
      "Input: [26, 13320] (['from', '23rd']) / Label: 0\n",
      "Input: [5092, 13440] (['martinez', \"milburn's\"]) / Label: 0\n",
      "Input: [127, 3264] (['off', 'expert']) / Label: 0\n",
      "Input: [2178, 511] (['veteran', 'needed']) / Label: 0\n",
      "Input: [4366, 30564] (['vinci', 'exclusions']) / Label: 0\n",
      "Input: [3, 30102] (['to', '30am']) / Label: 0\n",
      "Input: [567, 13850] (['beat', 'seminal']) / Label: 0\n",
      "Input: [3, 8685] (['to', 'pregnant']) / Label: 0\n",
      "Input: [5093, 8848] (['qatar', 'geldof']) / Label: 0\n",
      "Input: [1, 24870] (['', 'klein']) / Label: 0\n",
      "Input: [1729, 14220] (['italian', 'â£14bn']) / Label: 0\n",
      "Input: [267, 20119] (['came', \"needed'\"]) / Label: 0\n",
      "Input: [1361, 9400] (['t', 'vauxhall']) / Label: 0\n",
      "Input: [130, 32112] (['down', \"qca's\"]) / Label: 0\n",
      "Input: [1, 7084] (['', 'heralded']) / Label: 0\n",
      "Input: [110, 4902] (['set', '10th']) / Label: 0\n",
      "Input: [5093, 7537] (['qatar', 'literary']) / Label: 0\n",
      "Input: [12988, 4331] (['doha', '2009']) / Label: 0\n",
      "Input: [7, 23159] (['in', 'spurring']) / Label: 0\n",
      "Input: [567, 29652] (['beat', 'bobs']) / Label: 0\n",
      "Input: [4366, 8398] (['vinci', 'premature']) / Label: 0\n",
      "Input: [267, 21552] (['came', 'gmr']) / Label: 0\n",
      "Input: [250, 7703] (['open', 'rallied']) / Label: 0\n",
      "Input: [127, 3913] (['off', 'rating']) / Label: 0\n",
      "Input: [2178, 28696] (['veteran', 'hinton']) / Label: 0\n",
      "Input: [2, 14998] (['the', 'nasa']) / Label: 0\n",
      "Input: [250, 25195] (['open', 'mothercare']) / Label: 0\n",
      "Input: [130, 23825] (['down', 'organizational']) / Label: 0\n",
      "Input: [3826, 1503] (['spaniard', 'read']) / Label: 0\n",
      "Input: [945, 959] (['challenge', 'air']) / Label: 0\n",
      "Input: [4366, 8998] (['vinci', \"forsyth's\"]) / Label: 0\n",
      "Input: [110, 7842] (['set', 'notably']) / Label: 0\n",
      "Input: [945, 17441] (['challenge', 'prejudices']) / Label: 0\n",
      "Input: [1784, 27885] (['sees', 'farmhouse']) / Label: 0\n",
      "Input: [12988, 7319] (['doha', 'attached']) / Label: 0\n",
      "Input: [6, 19236] (['a', 'directconnect']) / Label: 0\n",
      "Input: [26, 10265] (['from', 'anticipation']) / Label: 0\n",
      "Input: [5092, 30010] (['martinez', 'texture']) / Label: 0\n",
      "Input: [1784, 24706] (['sees', 'asbestosis']) / Label: 0\n",
      "Input: [21, 25852] (['at', 'beheaded']) / Label: 0\n",
      "Input: [9001, 25671] (['conchita', 'disdain']) / Label: 0\n",
      "Input: [9001, 32356] (['conchita', 'umbilical']) / Label: 0\n",
      "Input: [5092, 22746] (['martinez', 'tacitly']) / Label: 0\n",
      "Input: [4366, 2978] (['vinci', 'lies']) / Label: 0\n",
      "Input: [3826, 3964] (['spaniard', 'errors']) / Label: 0\n",
      "Input: [7, 18941] (['in', 'tru']) / Label: 0\n",
      "Input: [6, 14286] (['a', 'dispensed']) / Label: 0\n",
      "Input: [2, 8981] (['the', 'deported']) / Label: 0\n",
      "Input: [21, 10393] (['at', \"nadal's\"]) / Label: 0\n",
      "Input: [1729, 24347] (['italian', 'deplored']) / Label: 0\n",
      "Input: [26, 25679] (['from', 'booy']) / Label: 0\n",
      "Input: [5092, 22978] (['martinez', 'tractor']) / Label: 0\n",
      "Input: [127, 27976] (['off', \"cash's\"]) / Label: 0\n",
      "Input: [2178, 68] (['veteran', 'now']) / Label: 0\n",
      "Input: [4366, 12097] (['vinci', 'cruises']) / Label: 0\n",
      "Input: [3, 16420] (['to', 'luca']) / Label: 0\n",
      "Input: [567, 14866] (['beat', 'hank']) / Label: 0\n",
      "Input: [3, 3686] (['to', 'presidential']) / Label: 0\n",
      "Input: [5093, 1948] (['qatar', 'partner']) / Label: 0\n",
      "Input: [1, 27274] (['', 'twiddling']) / Label: 0\n",
      "Input: [1729, 12249] (['italian', 'continents']) / Label: 0\n",
      "Input: [267, 16073] (['came', 'contamination']) / Label: 0\n",
      "Input: [1361, 8577] (['t', 'accomplished']) / Label: 0\n",
      "Input: [130, 23201] (['down', 'modestly']) / Label: 0\n",
      "Input: [1, 13214] (['', 'remy']) / Label: 0\n",
      "Input: [110, 30947] (['set', 'showings']) / Label: 0\n",
      "Input: [5093, 8267] (['qatar', 'defenders']) / Label: 0\n",
      "Input: [12988, 23045] (['doha', \"energy's\"]) / Label: 0\n",
      "Input: [7, 24317] (['in', 'ji']) / Label: 0\n",
      "Input: [567, 16877] (['beat', 'potatoes']) / Label: 0\n",
      "Input: [4366, 21744] (['vinci', \"souness'\"]) / Label: 0\n",
      "Input: [267, 7709] (['came', 'scepticism']) / Label: 0\n",
      "Input: [250, 16383] (['open', 'dished']) / Label: 0\n",
      "Input: [127, 4858] (['off', \"hollywood's\"]) / Label: 0\n",
      "Input: [2178, 17393] (['veteran', 'insured']) / Label: 0\n",
      "Input: [2, 20142] (['the', 'slums']) / Label: 0\n",
      "Input: [250, 10006] (['open', 'defamation']) / Label: 0\n",
      "Input: [130, 4961] (['down', 'restore']) / Label: 0\n",
      "Input: [3826, 30943] (['spaniard', 'aspire']) / Label: 0\n",
      "Input: [945, 15873] (['challenge', 'knack']) / Label: 0\n",
      "Input: [4366, 23305] (['vinci', '47m']) / Label: 0\n",
      "Input: [110, 11942] (['set', 'slew']) / Label: 0\n",
      "Input: [945, 7080] (['challenge', 'provisional']) / Label: 0\n",
      "Input: [1784, 5373] (['sees', 'grabbed']) / Label: 0\n",
      "Input: [12988, 24453] (['doha', '0730']) / Label: 0\n",
      "Input: [6, 17388] (['a', 'unsecured']) / Label: 0\n",
      "Input: [26, 31536] (['from', '100ml']) / Label: 0\n",
      "Input: [5092, 24802] (['martinez', 'renationalising']) / Label: 0\n",
      "Input: [1784, 10246] (['sees', 'crying']) / Label: 0\n",
      "Input: [21, 14217] (['at', 'accountant']) / Label: 0\n",
      "Input: [9001, 18935] (['conchita', 'bashir']) / Label: 0\n",
      "Input: [9001, 29954] (['conchita', 'rudolf']) / Label: 0\n",
      "Input: [5092, 19682] (['martinez', 'geek']) / Label: 0\n",
      "Input: [4366, 17804] (['vinci', 'demerger']) / Label: 0\n",
      "Input: [3826, 13523] (['spaniard', 'pickering']) / Label: 0\n",
      "Input: [7, 3667] (['in', 'dan']) / Label: 0\n",
      "Input: [6, 20259] (['a', 'scrutinising']) / Label: 0\n",
      "Input: [2, 13687] (['the', 'tokumasu']) / Label: 0\n",
      "Input: [21, 3410] (['at', 'maria']) / Label: 0\n",
      "Input: [1729, 6114] (['italian', 'ryanair']) / Label: 0\n",
      "Input: [26, 26529] (['from', 'rhymes']) / Label: 0\n",
      "Input: [5092, 27914] (['martinez', 'bravado']) / Label: 0\n",
      "Input: [127, 13607] (['off', 'flick']) / Label: 0\n",
      "Input: [2178, 13505] (['veteran', 'perpetrators']) / Label: 0\n",
      "Input: [4366, 14035] (['vinci', 'vido']) / Label: 0\n",
      "Input: [3, 24773] (['to', 'exile']) / Label: 0\n",
      "Input: [567, 6810] (['beat', 'advisor']) / Label: 0\n",
      "Input: [3, 24103] (['to', 'â£532m']) / Label: 0\n",
      "Input: [5093, 15723] (['qatar', 'stoddart']) / Label: 0\n",
      "Input: [1, 20994] (['', 'midfielders']) / Label: 0\n",
      "Input: [1729, 8931] (['italian', 'designers']) / Label: 0\n",
      "Input: [267, 7582] (['came', 'podcast']) / Label: 0\n",
      "Input: [1361, 20286] (['t', 'â£947m']) / Label: 0\n",
      "Input: [130, 15032] (['down', 'stereotype']) / Label: 0\n",
      "Input: [1, 5383] (['', 'stem']) / Label: 0\n",
      "Input: [110, 19789] (['set', 'playstations']) / Label: 0\n",
      "Input: [5093, 30263] (['qatar', 'lawless']) / Label: 0\n",
      "Input: [12988, 4239] (['doha', 'hopeful']) / Label: 0\n",
      "Input: [7, 28714] (['in', 'pilgrimage']) / Label: 0\n",
      "Input: [567, 21302] (['beat', 'bees']) / Label: 0\n",
      "Input: [4366, 25760] (['vinci', 'sapphire']) / Label: 0\n",
      "Input: [267, 24571] (['came', 'unhappiness']) / Label: 0\n",
      "Input: [250, 13573] (['open', 'heel']) / Label: 0\n",
      "Input: [127, 12608] (['off', 'eds']) / Label: 0\n",
      "Input: [2178, 13395] (['veteran', \"radcliffe's\"]) / Label: 0\n",
      "Input: [2, 11778] (['the', 'rigged']) / Label: 0\n",
      "Input: [250, 22111] (['open', 'oyedele']) / Label: 0\n",
      "Input: [130, 10864] (['down', 'queue']) / Label: 0\n",
      "Input: [3826, 23038] (['spaniard', '85bn']) / Label: 0\n",
      "Input: [945, 29887] (['challenge', 'cropping']) / Label: 0\n",
      "Input: [4366, 27840] (['vinci', 'spawn']) / Label: 0\n",
      "Input: [110, 13835] (['set', '32m']) / Label: 0\n",
      "Input: [945, 28783] (['challenge', 'groove']) / Label: 0\n",
      "Input: [1784, 11019] (['sees', \"motorola's\"]) / Label: 0\n",
      "Input: [12988, 11873] (['doha', 'boe']) / Label: 0\n",
      "Input: [6, 12257] (['a', 'rescued']) / Label: 0\n",
      "Input: [26, 9950] (['from', 'truancy']) / Label: 0\n",
      "Input: [5092, 15406] (['martinez', 'grandfather']) / Label: 0\n",
      "Input: [1784, 1422] (['sees', 'vice']) / Label: 0\n",
      "Input: [21, 25787] (['at', 'seductive']) / Label: 0\n",
      "Input: [9001, 22588] (['conchita', 'campo']) / Label: 0\n",
      "Input: [9001, 23738] (['conchita', 'resilient']) / Label: 0\n",
      "Input: [5092, 11610] (['martinez', 'penned']) / Label: 0\n",
      "Input: [4366, 6413] (['vinci', 'dividends']) / Label: 0\n",
      "Input: [3826, 12265] (['spaniard', 'â£39']) / Label: 0\n",
      "Input: [7, 19963] (['in', 'jarek']) / Label: 0\n",
      "Input: [6, 18572] (['a', 'rufus']) / Label: 0\n",
      "Input: [2, 5290] (['the', 'devon']) / Label: 0\n",
      "Input: [21, 29622] (['at', 'preppy']) / Label: 0\n",
      "Input: [1729, 21965] (['italian', '23secs']) / Label: 0\n",
      "Input: [26, 11929] (['from', 'nicotine']) / Label: 0\n",
      "Input: [5092, 6691] (['martinez', '90s']) / Label: 0\n",
      "Input: [127, 11748] (['off', 'gloucestershire']) / Label: 0\n",
      "Input: [2178, 29981] (['veteran', 'rasor']) / Label: 0\n",
      "Input: [4366, 24292] (['vinci', 'maximov']) / Label: 0\n",
      "Input: [3, 20762] (['to', 'beardsley']) / Label: 0\n",
      "Input: [567, 11620] (['beat', 'unfairly']) / Label: 0\n",
      "Input: [3, 11133] (['to', 'â£500m']) / Label: 0\n",
      "Input: [5093, 14718] (['qatar', 'remade']) / Label: 0\n",
      "Input: [1, 31657] (['', 'duration']) / Label: 0\n",
      "Input: [1729, 1221] (['italian', 'chinese']) / Label: 0\n",
      "Input: [267, 3777] (['came', 'typically']) / Label: 0\n",
      "Input: [1361, 6716] (['t', 'leamy']) / Label: 0\n",
      "Input: [130, 26079] (['down', 'smillie']) / Label: 0\n",
      "Input: [1, 8682] (['', 'reinforce']) / Label: 0\n",
      "Input: [110, 12078] (['set', 'fabrics']) / Label: 0\n",
      "Input: [5093, 7116] (['qatar', 'persico']) / Label: 0\n",
      "Input: [12988, 6490] (['doha', 'walsh']) / Label: 0\n",
      "Input: [7, 24540] (['in', 'deferring']) / Label: 0\n",
      "Input: [567, 1316] (['beat', 'id']) / Label: 0\n",
      "Input: [4366, 11280] (['vinci', 'philadelphia']) / Label: 0\n"
     ]
    }
   ],
   "source": [
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    tokenizer.texts_to_sequences([news_stories[0][:150]])[0],\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size, negative_samples=4, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "i = 0\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    i += 1\n",
    "    print(f\"Input: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Z-cv7BQdHh1z"
   },
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of 1 on either side of target word\n",
    "epochs = 5 # Number of epochs to train for\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid data points randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhkxO7wtHy5C",
    "outputId": "85fe7e45-790c-4d3b-be67-ec31b071feca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cbow_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " context_embedding (Embeddi  (None, 2, 128)               1920128   ['input_2[0][0]']             \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " target_embedding (Embeddin  (None, 128)                  1920128   ['input_1[0][0]']             \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 2)                    0         ['context_embedding[0][0]',   \n",
      "                                                                     'target_embedding[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3840256 (14.65 MB)\n",
      "Trainable params: 3840256 (14.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# Inputs; target input layer will have the final shape [None]\n",
    "# context will have [None, 2xwindow_size] shape\n",
    "input_1 = tf.keras.layers.Input(shape=())\n",
    "input_2 = tf.keras.layers.Input(shape=(window_size*2,))\n",
    "\n",
    "# Target and context embedding layers\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "\n",
    "# Outputs of the target and context embedding lookups\n",
    "context_out = context_embedding_layer(input_2)\n",
    "target_out = target_embedding_layer(input_1)\n",
    "\n",
    "# Taking the mean over the all the context words to produce [None, embedding_size]\n",
    "mean_context_out = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_out)\n",
    "\n",
    "# Computing the dot product between the two\n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "cbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='cbow_model')\n",
    "\n",
    "cbow_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cKPwKPvpI89V"
   },
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "\n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "\n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "\n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "\n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "\n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "\n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "\n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8MU-t7SiIg0u"
   },
   "outputs": [],
   "source": [
    "def cbow_data_generator(sequences, window_size, batch_size, negative_samples):\n",
    "\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "        inputs, labels = cbow_grams(\n",
    "            sequences[si],\n",
    "            vocabulary_size=n_vocab,\n",
    "            window_size=window_size,\n",
    "            negative_samples=negative_samples,\n",
    "            shuffle=True,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=None\n",
    "        )\n",
    "\n",
    "        inputs_context, inputs_target, labels = np.array([inp[1] for inp in inputs]), np.array([inp[0] for inp in inputs]), np.array(labels).reshape(-1,1)\n",
    "\n",
    "        assert inputs_context.shape[0] == inputs_target.shape[0]\n",
    "        assert inputs_context.shape[0] == labels.shape[0]\n",
    "\n",
    "        #print(inputs_context.shape, inputs_target.shape, labels.shape)\n",
    "        for eg_id_start in range(0, inputs_context.shape[0], batch_size):\n",
    "\n",
    "            yield (\n",
    "                inputs_target[eg_id_start: min(eg_id_start+batch_size, inputs_target.shape[0])],\n",
    "                inputs_context[eg_id_start: min(eg_id_start+batch_size, inputs_context.shape[0]),:]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8L0P1xjIhcG",
    "outputId": "1d5376c0-1be8-4150-922e-5e3a1ea236e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "   2226/Unknown - 106s 47ms/step - loss: 0.4626election: page, engineer, ruddock, attorney, per\n",
      "me: duty, victims, reducing, cultural, training\n",
      "with: cannot, guilty, holmes, surround, universal\n",
      "you: must, we, they, themselves, dubbed\n",
      "were: extremely, closer, so, then, always\n",
      "win: 31, liverpool, school, sideways, files\n",
      "those: these, model, semi, initially, names\n",
      "music: devices, road, knowledge, management, tim\n",
      "also: cannot, must, picture, buy, before\n",
      "third: st, mrs, savings, euros, dvds\n",
      "best: connect, nominees, whose, successfully, stars\n",
      "him: jan, airline, capability, models, leaves\n",
      "too: clash, william, sporting, wimbledon, weak\n",
      "into: model, cannot, 45, targets, declined\n",
      "through: â£1, names, accounting, parliamentary, while\n",
      "mr: gordon, tony, speaker, special, who\n",
      "response: trillion, footage, resort, historical, island\n",
      "kept: liverpool, overall, â£5, japan's, frank\n",
      "computers: track, loans, 800, reviews, accounting\n",
      "domestic: 2m, lee, police, replace, indian\n",
      "fast: starring, mario, historical, birthday, movie\n",
      "risk: jonathan, offences, completely, flat, welcomed\n",
      "person: previously, overall, richard, nominees, debate\n",
      "previously: seed, names, crowd, these, japan's\n",
      "campbell: outside, contracts, inquiry, 1994, alongside\n",
      "ability: savings, model, school, these, duty\n",
      "programs: 45, â£2, 35, jonathan, capability\n",
      "contract: stage, asking, behaviour, com, targets\n",
      "j: expression, harry, female, kostas, party's\n",
      "allowed: holmes, tommy, west, access, generally\n",
      "unit: latest, outside, fund, detailed, beat\n",
      "believed: non, planned, flat, site, ice\n",
      "\n",
      "\n",
      "2226/2226 [==============================] - 106s 47ms/step - loss: 0.4626\n",
      "Epoch: 2/5 started\n",
      "   2225/Unknown - 104s 47ms/step - loss: 0.3976election: attorney, hemisphere, late, strategist, production\n",
      "me: abc, hosts, anybody, identification, she's\n",
      "with: men, subscriber, lined, spike, path\n",
      "you: we, they, i, not, don't\n",
      "were: are, extremely, thought, then, see\n",
      "win: 59, artist, liverpool, relatives, provoked\n",
      "those: conversion, instantly, traditionally, terrorists, compensation\n",
      "music: audit, challenge, saturday's, borders, car\n",
      "also: already, never, always, me, now\n",
      "third: fourth, rising, near, citizens, collection\n",
      "best: jeff, industries, implementation, bid', efficiency\n",
      "him: ons, doesn't, usually, smoking, regularly\n",
      "too: very, probably, clearly, extremely, together\n",
      "into: signing, armed, command, fighter, derby\n",
      "through: compensation, 150, burger, pornography, depressed\n",
      "mr: tony, gordon, silk, charles, said\n",
      "response: holmes', greene, ceremony, pops, mozilla\n",
      "kept: across, elite, runner, rees, sensible\n",
      "computers: bubble, incorporating, 40m, triumph, calculating\n",
      "domestic: enterprise, dee, atlantic, 2m, barcelona\n",
      "fast: rent, mcconnell, mario, slowing, banner\n",
      "risk: nadal, problem, disappeared, thomson, yukos'\n",
      "person: winter, boateng, regarding, lived, rio\n",
      "previously: document, skype, starred, terrorists, adrian\n",
      "campbell: kay, dj, hey, dies, fernando\n",
      "ability: waste, incorporating, chair, pit, injunction\n",
      "programs: chairs, secretly, clichy, deposits, mauresmo\n",
      "contract: berlin, sars, involving, pub, speaker\n",
      "j: o, kay, dj, justin, n\n",
      "allowed: humphreys, tommy, mubanga, n, rafael\n",
      "unit: steve, novel, france's, damaging, diana\n",
      "believed: revealed, philosophy, warren, hope, estate\n",
      "\n",
      "\n",
      "2226/2226 [==============================] - 104s 47ms/step - loss: 0.3976\n",
      "Epoch: 3/5 started\n",
      "   2225/Unknown - 107s 48ms/step - loss: 0.3747election: weakening, elections, attorney, late, europe's\n",
      "me: him, them, adequately, thousands, certainly\n",
      "with: bills, visuals, atlantic, subscriber, together\n",
      "you: we, they, i, don't, still\n",
      "were: are, have, being, prepared, insist\n",
      "win: remember, missed, statements, liverpool, confident\n",
      "those: interested, partly, unfounded, councils, jamaican\n",
      "music: management, recorders, development, divide, member\n",
      "also: never, already, always, i've, we've\n",
      "third: fourth, second, 19th, motion, finals\n",
      "best: award, category, nomination, industries, gillian\n",
      "him: them, me, quickly, itself, themselves\n",
      "too: very, so, really, extremely, calm\n",
      "into: fighter, usage, armed, directly, command\n",
      "through: else, directly, topics, interested, immediately\n",
      "mr: tony, gordon, silk, bernie, hughes\n",
      "response: giggs, newly, occasion, page, candidate\n",
      "kept: dating, slaves, outright, exits, farrell's\n",
      "computers: triumph, complaints, chilean, page, bubble\n",
      "domestic: bernabeu, convertible, ones, group's, glazer's\n",
      "fast: holders, servat, revive, cynicism, withdrawn\n",
      "risk: bullying, gang, candidate, itv's, margins\n",
      "person: load, asymmetric, eyeing, cairns, brightest\n",
      "previously: never, already, actually, they've, we've\n",
      "campbell: dawson, pearson, kay, worsley, mcdonald\n",
      "ability: trades, margins, â£350m, incorporating, petered\n",
      "programs: whipped, balancing, rhythm, caught, reading\n",
      "contract: poetry, borne, singing, turkey's, surpassing\n",
      "j: o, saints, k, n, m\n",
      "allowed: enjoyed, repeatedly, hasn't, keane, stressed\n",
      "unit: employer, sweden's, violent, branding, brazilian\n",
      "believed: claimed, ed, wants, praised, speaks\n",
      "\n",
      "\n",
      "2226/2226 [==============================] - 107s 48ms/step - loss: 0.3747\n",
      "Epoch: 4/5 started\n",
      "   2225/Unknown - 106s 48ms/step - loss: 0.3547election: weakening, attorney, labour's, europe's, motors\n",
      "me: him, tories, them, myself, we'll\n",
      "with: visuals, bills, one's, atlantic, locking\n",
      "you: we, they, i, don't, certainly\n",
      "were: are, have, aren't, being, we're\n",
      "win: missed, confident, 155, 1m, 75\n",
      "those: interested, persecution, muslims, others, sweeteners\n",
      "music: recorders, images, download, divide, entertainment\n",
      "also: already, i've, we've, never, always\n",
      "third: fourth, second, prototype, finals, fifth\n",
      "best: award, gillian, category, nomination, industries\n",
      "him: me, them, myself, tories, themselves\n",
      "too: very, so, extremely, increasingly, quite\n",
      "into: forward, ahead, through, usage, outside\n",
      "through: drawn, directly, usage, forward, visitors\n",
      "mr: tony, silk, gordon, bernie, hughes\n",
      "response: launch, settlement, siemens, challenge, deaths\n",
      "kept: cardinal, bumper, taipei, exits, kezman\n",
      "computers: barcelona, tracks, sketch, siemens, homeland\n",
      "domestic: retailer, liffe, portfolio, korean, country's\n",
      "fast: timely, reception, plaster, sustaining, explanations\n",
      "risk: feedback, agents, referee's, balado, gang\n",
      "person: 91, 10m, penny, radius, destroy\n",
      "previously: i've, they've, actually, already, you've\n",
      "campbell: smith, dies, boss, knight, milburn\n",
      "ability: accomplished, bafin, flushing, fellowship, carlsberg\n",
      "programs: midday, balancing, method, edonkey, stubborn\n",
      "contract: occupation, brett, remake, mull, salaries\n",
      "j: o, m, rowntree, k, p\n",
      "allowed: enjoyed, kasabian, cyril, cleared, depress\n",
      "unit: sports, decorated, banking, aerospace, pc\n",
      "believed: claimed, insisted, admitted, knew, pointed\n",
      "\n",
      "\n",
      "2226/2226 [==============================] - 106s 48ms/step - loss: 0.3547\n",
      "Epoch: 5/5 started\n",
      "   2226/Unknown - 107s 48ms/step - loss: 0.3382election: motors, labour's, attorney, trail, arrogance\n",
      "me: him, them, myself, everyone, tories\n",
      "with: atlantic, constructive, one's, bills, visuals\n",
      "you: we, i, i'll, they, don't\n",
      "were: are, locking, we're, insist, aren't\n",
      "win: 75, 155, â£10, victories, victory\n",
      "those: interested, others, persecution, individuals, they're\n",
      "music: cameras, gadget, recorders, images, divide\n",
      "also: repeatedly, previously, already, never, always\n",
      "third: fourth, second, first, fifth, sixth\n",
      "best: nomination, category, award, contenders, prizes\n",
      "him: them, me, myself, themselves, automatically\n",
      "too: very, extremely, increasingly, pretty, so\n",
      "into: forward, outside, through, dig, catalogue\n",
      "through: forward, drawn, directly, batteries, hammers\n",
      "mr: tony, silk, gordon, bernie, acknowledged\n",
      "response: cycle, 1993, origin, engaging, 300p\n",
      "kept: chairs, imported, pulled, exposure, prevent\n",
      "computers: barcelona, homeland, sketch, regulators, volley\n",
      "domestic: state's, country's, entire, gross, industrial\n",
      "fast: luck, africans, timely, bits, speeds\n",
      "risk: problem, agents, feedback, transplant, candidate\n",
      "person: victories, threaten, frantic, penny, brightest\n",
      "previously: they've, never, i've, already, we've\n",
      "campbell: smith, mcdonald, defender, humphreys, worsley\n",
      "ability: deterioration, accomplished, flushing, advantages, carlsberg\n",
      "programs: tool, edonkey, links, voip, banning\n",
      "contract: sensitivity, pakistani, crown, filmmaking, appearance\n",
      "j: m, o, saints, k, p\n",
      "allowed: enjoyed, nhtcu, switching, pg, autolink\n",
      "unit: menatep, owned, sports, parent, aerospace\n",
      "believed: claimed, admitted, insisted, warned, welcomed\n",
      "\n",
      "\n",
      "2226/2226 [==============================] - 107s 48ms/step - loss: 0.3382\n"
     ]
    }
   ],
   "source": [
    "cbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    news_cbow_gen = cbow_data_generator(news_sequences, window_size, batch_size, negative_samples)\n",
    "    cbow_model.fit(\n",
    "        news_cbow_gen,\n",
    "        epochs=1,\n",
    "        callbacks=cbow_validation_callback,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
