{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Named Entity Recognition with RNNs\n",
    "This task is known as named entity recognition (NER). We’ll be using a modified version of the well-known Conference on Computational Natural Language Learning 2003 (CoNLL 2003) dataset for NER.It’s a more closely curated version than the original CoNLL, which contains errors in the dataset induced by incorrectly understanding the context of a word. For example, in the phrase “Chicago won ...” Chicago was identified as a location, whereas it’s actually an organization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adjustText\n",
      "  Downloading adjustText-1.2.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from adjustText) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from adjustText) (3.9.2)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from adjustText) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->adjustText) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n",
      "Downloading adjustText-1.2.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: adjustText\n",
      "Successfully installed adjustText-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "# !pip install adjustText\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conllpp_dev.txt already exists.\n",
      "conllpp_test.txt already exists.\n",
      "conllpp_train.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_data(urls, data_dir):\n",
    "    \"\"\"Download specified files if not present.\"\"\"\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for file_name in urls:\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f'Downloading {file_name}...')\n",
    "            response = requests.get(urls[file_name])\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f'{file_name} downloaded successfully.')\n",
    "        else:\n",
    "            print(f\"{file_name} already exists.\")\n",
    "\n",
    "# URLs of the files to download\n",
    "urls = {\n",
    "    'conllpp_dev.txt': 'https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/master/data/conllpp_dev.txt',\n",
    "    'conllpp_test.txt': 'https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/master/data/conllpp_test.txt',\n",
    "    'conllpp_train.txt': 'https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/master/data/conllpp_train.txt'\n",
    "}\n",
    "\n",
    "download_data(urls, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given file name\n",
    "    Returns a list of sentences (each sentence a string) \n",
    "    and list of ner labels for each string\n",
    "    '''\n",
    "\n",
    "    print(\"Reading data ...\")\n",
    "    # Manager lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
    "    sentences, ner_labels = [], [] \n",
    "    \n",
    "    # Open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
    "        \n",
    "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                is_sos = False\n",
    "            # Otherwise keep capturing tokens and labels\n",
    "            else:\n",
    "                is_sos = True\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            \n",
    "            # When we reach the end or reach the beginning of next\n",
    "            # Add the data to the manager lists, flush the temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    print('\\tDone')\n",
    "    return sentences, ner_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: LONDON 1996-08-30\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_sentences, train_labels = read_data('data/conllpp_train.txt') \n",
    "# Validation data\n",
    "valid_sentences, valid_labels = read_data('data/conllpp_dev.txt') \n",
    "# Test data\n",
    "test_sentences, test_labels = read_data('data/conllpp_test.txt') \n",
    "\n",
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
    "    print(f\"Sentence: {v_sent}\")\n",
    "    print(f\"Labels: {v_labels}\")\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label counts\n",
      "O         169578\n",
      "B-LOC       7140\n",
      "B-PER       6600\n",
      "B-ORG       6321\n",
      "I-PER       4528\n",
      "I-ORG       3704\n",
      "B-MISC      3438\n",
      "I-LOC       1157\n",
      "I-MISC      1155\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Print the value count for each label\n",
    "print(\"Training data label counts\")\n",
    "print(pd.Series(chain(*train_labels)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14041.000000\n",
       "mean        14.501887\n",
       "std         11.602756\n",
       "min          1.000000\n",
       "5%           2.000000\n",
       "50%         10.000000\n",
       "95%         37.000000\n",
       "max        113.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class label -> class ID mapping\n",
    "    labels_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    print(f\"labels_map: {labels_map}\")\n",
    "    return labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "labels_map = get_label_id_map(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_int_labels(labels, labels_map, max_seq_length, return_mask=True):\n",
    "\n",
    "    # Convert string labels to integers \n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and \n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                int_labels, maxlen=max_seq_length, padding='post', truncating='post', value=-1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    \n",
    "    else:\n",
    "        padded_labels = np.array(ner_pad_sequence_func(int_labels, value=labels_map['O']))\n",
    "        return padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to integers for all train/validation/test data\n",
    "# Pad train/validation/test data\n",
    "padded_train_labels, train_mask = get_padded_int_labels(train_labels, labels_map, max_seq_length, return_mask=True)\n",
    "\n",
    "padded_valid_labels, valid_mask = get_padded_int_labels(valid_labels, labels_map, max_seq_length, return_mask=True)\n",
    "\n",
    "padded_test_labels, test_mask  = get_padded_int_labels(test_labels, labels_map, max_seq_length, return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]\n",
      " [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]]\n",
      "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Print some labels IDs\n",
    "print(padded_train_labels[:2])\n",
    "print(train_mask[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
